{"pageProps":{"member":{"id":"nwiizo","name":"nwiizo","role":"Software Developer","bio":"Brogrammer","avatarSrc":"/avatars/nwiizo.jpeg","sources":["https://syu-m-5151.hatenablog.com/feed","https://zenn.dev/nwiizo/feed"],"includeUrlRegex":"","twitterUsername":"nwiizo","githubUsername":"nwiizo","websiteUrl":"https://nwiizo.github.io/"},"postItems":[{"title":"インフラエンジニアが学ぶと良さそうなgRPCサーバーについて","contentSnippet":"3-shake にはSreake共有会 という毎週、火曜日と木曜日に担当者が現場で得た知見などを発表する社内勉強会が開催されています。こちらのブログはそれらを変更修正しております。syu-m-5151.hatenablog.com元々しようとしていたの話Go 1.18 の最新情報←Generics の深い話とかはもう既出すぎて気になる人は読んでるGo でのTDD(が実は20周年なので)←書いてる途中で自分が言うべきことなんてないことに気付く今後、案件で増えるであろう gRPC についてインフラエンジニアが知っておいても良いと思ったという話 ← 今ここTL;DRgRPC の概要とインフラエンジニアが知っておくべきことについてgRPC をライブラリやツール、トレンドなどを調査grpc.ioRPCとはgRPC がこの世の中に急に爆誕したわけではない。そもそも、サービス間での情報のやり取りをどのように行うかというのは古くからある課題の1つです。その中で利用されているのがRPCがあります。RPCとは、Remote Procedure Callの略でと遠隔手続き呼出しと訳されます。すなわち、別の場所にあるプログラムを呼び出そうというのを目的としています。違うアプリケーションロジックをあたかも自分のアプリケーションの処理と同じように扱えることができるというのも特徴です。クライアントはサーバーに対し実行する処理を指定するパラメータや引数として与えるデータを送信し、それに対しサーバーはパラメータに応じた処理を実行してその結果をクライアントに返す、というのがRPCの基本的な流れになります。ちなみに、gRPC以外にもJSON-RPC、XML-RPCなどがあります。gRPCの歴史gRPCは、Googleが開発したRPC技術がベースとなっている。Googleでは多数のコンポーネントを組み合わせてサービスを実現しています。いわゆる、マイクロサービスアーキテクチャでシステムを構築していることで知られるが、これらのサービス間で通信を行うためにgRPCの前身Stubbyと呼ばれる技術が開発されました。ただ、StubbyはGoogleのインフラ以外での利用は想定しておらず、独自の仕様が多く、Stubby で使用されていた技術とコンセプトが近いHTTP/2 などの技術が登場したことから、GoogleはStubbyにこれらの技術を取り入れてオープン化することを決め、それがgRPCです。なお、現在ではgRPCはオープンソースで公開されており、現在はLinux Foundation傘下のCloud Native Computing Foundation（CNCF）によって開発が進められています。grpc.iogRPC についてgRPCではほかのRPCと同様、クライアントがサーバーに対しリクエストを送信し、サーバーはそれに応じた処理を実行してその結果を返すという、クライアント−サーバーモデルを採用している。gRPCでは以下のような特徴があります。HTTP/2 による高速な通信バイナリにシリアライズされて送られてくる小さな容量で転送できる一つのコネクションで複数のres/reqが可能(柔軟なストリーミング形式)ミドルウェアの設定でハマった時にはHTTP/2 の問題なのかgRPCの問題なのか切り分ける必要があると思います。Protocol buffersgRPCではProtocol Buffersのサービス定義ファイルからサーバーおよびクライアント向けのコードを自動的に生成するツールが提供されており、これを利用することで簡単にサーバーおよびクライアントを実装できるようになっている。そのため、クライアントとサーバーが異なる言語で実装されていても、問題なく通信を行うことができるようになっている。クライアント・サーバー間の通信に使用するプロトコル（トランスポート）や、やり取りするデータの表現およびシリアライズ方法については置き換えが可能な設計になっているが、デフォルトではトランスポートにHTTP/2が、データのシリアライズにはProtocol Buffersという技術を使用するようになっており、これをそのまま使用するのが一般的です。Protocol BuffersはGoogleが開発したデータフォーマットで、バイナリデータを含むデータでも効率的に扱えるのが特徴です。このProtocol Buffersについても、さまざまなプラットフォーム・プログラミング言語から利用できるライブラリが提供されている。柔軟なストリーミング形式単方向/双方向ストリーミングRPCに対応している。Unary RPC1つのリクエストに対して一つのレスポンスを返す一般的な通信です。誤解を恐れぬ言い方をするとREST API のような挙動。Server streaming RPCクライアントから送られてきた一つのリクエストに対して、複数回に分けてレスポンスを返す通信方式です。最後のレスポンスを返した後も任意にサーバーの情報を変更に応じてクライアントにその情報を送ることができます。Client streaming RPCクライアントからリクエストを分割して送ってサーバーはすべてのリクエストを受け取ってからレスポンスを返します。大きなデータをPOSTしたいときに便利です。Bidirectional streaming RPCクライアントからリクエストが送られてきたときにサーバーとクライアントは一つのコネクションを確立しお互いに任意のタイミングでリクエストとレスポンスを送りあうことが可能になります。他のプロトコルとの違いと連携Web サービスやマイクロサービスで使われるプロトコルの代表格は HTTP/HTTPS と、それを利用した REST API です。 HTTP は非常に柔軟ですが、渡すデータのスキーマが標準化されていないため、異なる言語間の RPC を実装するのは面倒です。 OpenAPI という REST API 用の IDL もありますが、Protocol Buffers と比較すると記述量が多いです。よく言われるのが、GraphQL です。GraphQL は Facebook が開発したプロトコルで、HTTP 上で処理されますが REST API とは異なり GET/POST などのメソッドやステータスコードに意味を持たせていません。 特徴はスキーマはデータ構造を定義するもので、標準化されたクエリにより任意のデータを取得可能な仕組みになっていることです。gRPC がどのようなものか？gRPC Motivation and Design Principles によればgRPCの基本的なコンセプトとして次のものが挙げられている。サービスはオブジェクトではなく、メッセージはリファレンス（参照）ではない適切な適用範囲とシンプルさフリーかつオープン相互運用性があり、一般的なインターネットインフラ内で利用できる汎用性がありながら、専用のものと比べてパフォーマンス面で一般に劣らないアプリケーションレイヤーと分離された構造ペイロードを問わないストリーミングでの情報伝達に対応同期・非同期の両方に対応通信の中断やタイムアウトをサポート確立された通信を処理しつつ新規接続を止めるようなシャットダウンのサポートデータ流量のコントロール機能デフォルト実装に対して後からさまざまな機能を追加可能APIによる機能拡張が可能メタデータの交換をサポート標準化されたステータスコードこのようなものを頭に叩き込んでいると様々な場面でgRPCの設計がどのような思考でそのようになされているか分かる。gRPC Ecosystemgithub.comgRPCを補完するgRPCエコシステムとして各種サービスが紹介されている。ヘルスチェックやPrometheus での設定などがこちらに紹介されているgRPC-WebgRPC-WebによってgRPC通信をWebでも使うことができる。HTTPサーバーが仲介者として機能することなく、WebアプリがgRPCバックエンドサービスと直接通信できるようになるものです。またクライアントもバックエンドもgRPCでの実装なので完全なエンドツーエンドのgRPCサービスアーキテクチャを作成できることが利点です。protoファイルに記述したらあとは、お互い実装ができるので開発も進められやすいです。github.comgRPC-Gatewayprotoファイルに書かれたサービスの定義を理解し、REST APIに変換できます 。gRPC-GatewayだけでRESTfulなAPIを受け取れます。また、protoファイルからswagger.jsonを自動出力してくれる機能も備わっており、ドキュメント生成に関しても十分です。grpc-ecosystem.github.ioenvoygRPC-GatewayとenvoyはどちらもJsonをgRPCに変換してくれる機能を持ち合わせています。JSONを変換してくれるだけよくGolangでの実装だったら、gRPC-Gatewayでいいのかなと思いますがそれ以外にはEnvoy にはさまざまな機能があるので一気に全部やってしまいたい方にはEnvoyの利用を考えても良いのかな？と思います。www.envoyproxy.iogRPC をライブラリやツールについてインフラエンジニアがgRPC に関わる時は開発というより運用や保守に関してだろう。なので、今回、紹介するツールもそれらに沿って紹介したい。ツールの使い方を調べれば自ずとgRPCの輪郭が見えてくるかと思います。Awesome gRPC はgRPC に 関するキュレーションを行うリポジトリ。大体のツールはここを確認すれば良い。https://github.com/grpc-ecosystem/awesome-grpcgrpc_cligrpc/command_line_tool.md at master · grpc/grpcgRPC の公式リポジトリに同梱されている grpc_cli は公式の gRPC クライアントツールといえますが、最低限の機能しか備えていません。例えば他の gRPC クライアントツールではほぼ実装されているメタデータの送信ができない、JSON 形式でのリクエスト内容の記述を受け付けられないといった問題があります。また、インストールするためにはソースコードからビルドする必要があり煩雑なのであまり、使われていません。gRPCurlhttps://github.com/fullstorydev/grpcurl最も使われている gRPC クライアントツールです。現在も活発にメンテナンスされています。機能面でもたいていのユースケースは網羅されており、機能の不足で困るようなことはほとんどないでしょう。prototoolhttps://github.com/uber/prototoolPrototoolは Uber Technologies によって開発された Protocol Buffers のユーティリティツールです。Prototool には gRPC のエンドポイントを呼び出せるサブコマンドが付属しています。ただし、このサブコマンドは fullstorydev/grpcurl に大きく依存しており、実質 gRPCurl のサブセットとなっています。現在は Protocol Buffers のユーティリティツールとして Buf を推奨するしています。Bufhttps://github.com/bufbuild/bufProtocol Buffers のユーティリティツール 戦争に勝ち抜いたと言っても良い buf は自動ファイル検出、正確なlintとbreaking checkersの構成を選択することができたり、エラー出力はどのエディターでも簡単に解析可能(vs Code はさまざまなツールが動くが、vim はこれぐらいしか、プラグインがうまく動かない)、コンパイルの高速化、protocのプロトコルプラグインとして使用する。gRPCUIhttps://github.com/fullstorydev/grpcuigrpcuiは、ブラウザ経由でgRPCサーバと対話するためのコマンドラインツールです。Postman のようなものですが、REST ではなく gRPC API のためのものです。evansgRPC クライアントツールです。REPL モードで手軽に手動テストを行えますのでデバッグの時にあるとめちゃくちゃ便利です。https://github.com/ktr0731/evansJSON-to-ProtoJSONを即座にProtobufに変換してくれるツールになります。JSON-to-Proto次回予告:gRPC を使ったアプリケーション開発の流れそれでは、gRPCを使ったアプリケーション開発を行う場合、実際にどのような手順を踏めば良いかを紹介していこう。この場合の基本的な流れは次のようになる。Protocol Buffersを使ったサービスの定義サービス定義ファイルからのコードの生成生成したコードに独自の自前の実装を追加する上記に関してはハンズオンなどで実施していきたいと思います。また、2022年4月27日に「Protocol Buffers/gRPC を安全に書き進めるためのエトセトラ」と題してOWASP Fukuoka Meeting #6にて登壇いたしますーowasp-kyushu.connpass.com参考文献公式資料grpc.iogRPC の公式サイトです。仕様だけでなく、各言語のチュートリアルもあります。grpc.github.io詳細なドキュメント群です。gRPC over HTTP2上記サイトの一ドキュメントです。HTTP/2 をどう利用しているかの仕様書です。developers.google.com/protocol-buffersProtocol Buffers の公式サイトです。The complete gRPC courseGoとJavaで開発できるチュートリアルです。gRPC: Up and RunninggRPC と Protocol Buffers の本です。Securing your gRPCApplicationKubeCon 2019 NA のセッションの一つで、gRPC の認証・認可の実装方法を詳しく解説しています。","link":"https://syu-m-5151.hatenablog.com/entry/2022/04/12/130411","isoDate":"2022-04-12T04:04:11.000Z","dateMiliSeconds":1649736251000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"samber / lo はGoperである私を愚直なfor もしくは筋肉信仰から救ってくれるのか？","contentSnippet":"はじめにGo 1.18 がリリースされました。Go 1.18でシュッとGenerics を手軽に良さを実感する方法としてsamber/lo があります。  もちろん、Tutorial: Getting started with generics で完全に理解できるならそちらの方が良いですし、これを終わった後でやることも推奨です。その他のリリースパーティや勉強会もとても勉強になりますが とにかく、samber/lo 便利なので紹介させてください！！！！！！！go.dev今回はとても大きな変更です。Generics が入りました。構成としては2つ。Type parameterType sets参考資料DevFest Tokyo 2021 でmattn さんが発表したスライド＆動画がとても分かりやすいので是非、見てみてください。docs.google.comwww.youtube.com全てfor 文で解決するのか？- そう、全て筋肉が解決してくれるGolang にはuniq メゾットのようなものがなく、重複のある slice に対して独自に処理を実装しなければいけなかった。愚直にfor を回すの結果として最速だからである。arr := []string{\"Samuel\", \"Marc\", \"Samuel\"}m := map[string]bool{}for _, ele := range arr {    if !m[ele] {        m[ele] = true        uniq = append(uniq, ele)    }}fmt.Printf(\"%v\", uniq) // [\"Samuel\", \"Marc\"]Go Playground - The Go Programming Languageどういうことかというと、重複キーがあるので、同様のキーを持つmapの場合は新しく値を上書きしないみたいな処理を書かなければならなかった。m[\"Samuel\"] = true は一度目はこれが呼ばれるけど、二度目はすでにtrueなので if句の中に入ってず、resultにSamuelが二度入ることがないという様な仕組みです。とにかく、全てをfor で扱い全ての型を制御するマッチョでした。ema-hiro.hatenablog.com全てfor 文で解決するのか？- samber/lo とか？Golang にはuniq メゾットのようなものがなく、重複のある slice に対して独自に処理を実装しなければいけなかったがsamber/lo というプロジェクトではGo 1.18 のGenerics を使うことによってreflect より早くforとも遜色なく動作するヘルパーを提供します。他にもいくつもの ヘルパー がありますが今回はuniq のみ紹介します。pkg.go.devpackage mainimport (    \"fmt\"    \"github.com/samber/lo\")func main() {    arr := []string{\"Samuel\", \"Marc\", \"Samuel\"}    names := lo.Uniq[string](arr)       fmt.Println(names) // []string{\"Samuel\", \"Marc\"}}uniqValues := lo.Uniq[int]([]int{1, 2, 2, 1})// []int{1, 2}実装をみるとこんな感じでmapと空のstructを使う方法でuniq が実装されている。lo/slice.go at v1.10.1 · samber/lo · GitHubfunc Uniq[T comparable](collection []T) []T {    result := make([]T, 0, len(collection))    seen := make(map[T]struct{}, len(collection))    for _, item := range collection {        if _, ok := seen[item]; ok {            continue        }        seen[item] = struct{}{}        result = append(result, item)    }    return result}とにかく、for で愚直に回す言語から多少はスマートな解決ができる様になった(もしくは今後、期待ができる様になった)。最後にこの記事を読んで興味が湧いたら元のProposalやTutorial: Getting started with generics を読んでみてください。自分も何度かやってみて読んでみて使える様になりたいと思ってます。また、Go本体にも機能として追加される日を楽しみしてます。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/03/16/122810","isoDate":"2022-03-16T03:28:10.000Z","dateMiliSeconds":1647401290000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"3-shake エンジニアのブログ記事まとめサイト「3-shake Engineers' Blogs」を公開しました。","contentSnippet":"3-shake Engineers' Blogs、爆誕す！3-shake inc. に所属するエンジニアが執筆したブログ記事をまとめたサイト、3-shake Engineers' Blogsを公開しました。blog.3-shake.comこちらは、@catnoseさんがOSSとして公開している、team-blog-hubをfork させていただき、Ubie さんのUbie Engineers' Blogsを参考にして作成いたしました。なぜ、作ったのか？3-shake には現在、公式のテックブログ(Sreake のブログ | sreake.com | 株式会社スリーシェイク というブログ)があります。が、メンバーが自発的にブログをポストしているわけではありません(別に良い悪いではなく)。理由はいろいろあると思いますが、テックブログは続かない - 何サイトか潰した後にブログが有名な企業に転職しての気づきと反省｜久松剛／IT百物語の蒐集家｜note にあるようないくつかの要素が原因かと思っています。が、3-shake がアウトプットしない文化という訳では決してありません。3-shake には現在、Sreake共有会 という毎週、火曜日と木曜日に担当者が現場で得た知見や調査した内容を発表する社内勉強会が開催されておます。これのポストは既に100件近く内部資料として溜まっており、レベルも相応に高いです。それらを対外的なアプトプットとして出せて、かつ、個人のブログでアウトプットしたほうがアウトプットするモチベーションも上がるのでは？という考えのもとに作成いたしました。最後にこれらの取り組みが3-shake を知っていただけることに多少なりとも繋がれば良いと思います。ちなみに、リポジトリをfork した後に社内調整をして、公開までいたしました。社会人力の低さを感じましたが3-shake が大切にしている価値観として5倍速というのがあるので許される気がしてます。@nwiizo さんのご尽力もあり、流行りに乗っかってみました笑うちのメンバーのブログをぜひ見てみてくださいー— TakuyaTezuka@3-shake (@tt0603) 2022年3月15日  告知また、3-shake で働くことに興味がある方は、採用サイトやホームページに詳しい情報を掲載していますのでご覧くださいwww.wantedly.com今週の金曜日の2022年3月18日に 3-shake SRE Tech Talk #3 というイベントがあって技術顧問 のまつもとりーさんが「コンテナの研究開発から学ぶLinuxの要素技術」と題してお話してくれるので皆様にも参加してほしいです3-shake.connpass.com参考資料zenn.devnote.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/03/15/153309","isoDate":"2022-03-15T06:33:09.000Z","dateMiliSeconds":1647325989000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Observability Conference 2022 に登壇しました","contentSnippet":"「Dapr の概念と実装から学ぶ Observability への招待」 というタイトルで登壇します。https://event.cloudnativedays.jp/o11y2022/talks/1382:embed:cite セッション概要Dapr は CloudNative な技術を背景に持つ分散アプリケーションランタイムです。本セッションでは Dapr の Observability に関する各種機能と、その実装について解説していきます。さらにスリーシェイクの Dapr と Observability への取り組みに関してもご紹介します。Dapr の機能でカバーできる点...","link":"https://zenn.dev/nwiizo/articles/d837b78914de23","isoDate":"2022-03-11T04:02:18.000Z","dateMiliSeconds":1646971338000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kind を利用してFeature Gates のGRPCContainerProbe を有効にしたKubernetes クラスターを構築してアルファー機能のgRPC Health Checkを試したいなー","contentSnippet":"Kind でGRPCContainerProbe がやりたいよはじめにKubernetesではLiveness & Readiness Probeを使って、Pod内のコンテナ、プロセスのヘルスチェックが行える。Kubernetes上で動くgRPCサーバーのヘルスチェックする際にはgrpc-health-probeで簡単に実装できます。readinessProbe やlivenessProbe,startupProbeにexec のcommand として実装する必要がある。どういうような方式があってみたいなのはHealth checking gRPC servers on Kubernetes | Kubernetes を参照していただければと思います。現行の場合にはこのように設定が必要  containers:  - name: server    image: \"[YOUR-DOCKER-IMAGE]\"    ports:    - containerPort: 5000    readinessProbe:      exec:        command: [\"/bin/grpc_health_probe\", \"-addr=:5000\"]      initialDelaySeconds: 5    livenessProbe:      exec:        command: [\"/bin/grpc_health_probe\", \"-addr=:5000\"]      initialDelaySeconds: 10tomioka-shogorila.hatenablog.comgRPC health checking が alpha feature として追加この、方式ではDockerfile内 にgrpc_health_probeを入れなくてはいけません。で、2022年2月で最新のKubernetes v1.23 では built-in gRPC health checking が alpha featureとして追加されました(同僚に教えてもらいました)。kubernetes.ioそのため、Kubernetes上で動くgRPCサーバーのヘルスチェックする際にbuilt-in でできるようになりました。  containers:  - name: server    image: \"[YOUR-DOCKER-IMAGE]\"    ports:    - containerPort: 5000    readinessProbe:            grpc:              port: 5000    livenessProbe:            grpc:              port: 5000しかし、これらの機能はまだ、alpha feature で機能 です。なので、defaultでは無効です。もし、試したい場合には--feature-gates として有効にしてあげればならないkubernetes.ioローカルでKubernetes クラスターを構築するにはいくつか方法があるのですが今回は、Kind を利用しているので今回もこちらを利用する。Kind でFeature Gates を利用するにはyaml で以下のように true にすることで適応できる。今回はGRPCContainerProbeをtrue にすれば良い。kubeadm でできることはKind でも大体できるのでkubeadmConfigPatches みたいな設定も忘れたくない。kind: ClusterapiVersion: kind.x-k8s.io/v1alpha4name: featuregatesnodes:- role: control-plane  image: kindest/node:v1.23.3@sha256:0df8215895129c0d3221cda19847d1296c4f29ec93487339149333bd9d899e5afeatureGates:  GRPCContainerProbe: truegithub.com最後にO11yCon での資料作成があるのにブログを書いてしまった。試験前に漫画を読み始めるみたいな感じでブログを書いてしまった。13歳からやってることは変わりません。お時間があれば見に来てください。event.cloudnativedays.jp","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/22/112713","isoDate":"2022-02-22T02:27:13.000Z","dateMiliSeconds":1645496833000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"CloudNativeな時代に求められるWebサービス基盤モデルの再考 というタイトルで Developers Summit 2022 に登壇しました。  ","contentSnippet":"振り返りDevelopers Summit 2022 で登壇しました。CloudNativeな時代に求められるWebサービス基盤モデルの再考 - Daprについての考察と実装 というタイトルで、CloudNativeな技術と共に歩んできた中で見えてきた、CloudNativeな技術を背景に持つ分散アプリケーションランタイムであるDaprがどういったもので何ができるかを解説するのを通してCloudNativeの必要性や立ち位置について発表しました。event.shoeisha.jp今回の発表でもっと喋りたいと思ったのはDapr によって Complexity は解消されるのか？に関する部分でRepositoryが何をするのかとDapr になります。実は来週もDapr での登壇イベントがあるのでRepository 周りの話と実装面の話を重点的にさせていただきたいと思います。jjug.doorkeeper.jp登壇資料体調が悪いのに頑張って資料作りましたね。偉いです。 speakerdeck.com文字起こししておいてます。意味はないです。振り返り登壇資料自動化とコンテナの話Infrastructure as Document 時代システム運用担当者がアプリケーションを配置する方法Infrastructure as Code 時代システム運用担当者がアプリケーションをコードによって配置する約束された勝利の自動化なんてない自動化の不都合な真実最初はうまくいく腐らないようにする自動化Immutable Infrastructure自動化を進めていく中で発見された素晴らしいプラクティスコンテナへの道アプリに必要なものを全て特定のフォーマットで固めて、展開するだけで起動自動化とコンテナのざっくりとした関係インフラの腐敗を防止クラウドによるアプリケーションファーストクラウドにより組み上げ方式から呼び出し方式へコンテナによって取り扱いを共通化Cloud NativeとWebサービス基盤モデルについて再考Service Level indicator とService Level Objective信頼性 100%と、信頼性 99.99%では大きな違い各サービスごとに適切なSLOを設定することが大事Service Level Indicator の例シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯCloud Native とは？疎結合ではないシステムとは？AvailabilityScalabilityComplexity疎結合なシステムとは？ResilientFlexible ScaleSimplicityKubernetes の特徴不変なインフラ宣言的設定自己修復機能Webサービス基盤モデルについてサービス層ストラテジー層オーケストレーション層コンテナランタイム層インフラストラクチャ層ストラテジー層の進化と拡大istioKnativeストラテジー層の拡大とDapr についてDapr とはComplexityとDapr の実装についてDapr とは？Daprの特徴Goalサイドカー パターンとは？分散システムにおけるデザインパターンの一つDapr におけるサイドカーアーキテクチャDapr の多様性ビルディングブロックビルディングブロックコンポーネントRepository において Dapr による抽象化の理想と現実Repository とはDapr によるRepository の吸収しきるのか？何を抽象化しているのか？Dapr によって Complexity は解消されるのか？Complexity とはDaprでComplexity の解消の夢は見れるか？参考資料自動化とコンテナの話Infrastructure as Document 時代システム運用担当者がアプリケーションを配置する方法人間の思考や行動をコーディングしてシステムを管理する為の秘伝のドキュメントを引き継ぐ一台一台を丁寧に設定していくが出自は不明なぜ、動いているかも分からず、さわったら動かなくなるから秘伝のタレとなりいつか腐るDocument はあるが更新されてないこともしばしばInfrastructure as Code 時代システム運用担当者がアプリケーションをコードによって配置する直訳すると「コードとしてのインフラ」、「インフラをコードで記述する」ことさまざまな手順や前提をCodeの中で表現していくことで全てが見えてくる黎明期はシステム管理の自動化がその後はソフトウェア開発プラクティスを応用するのが焦点に約束された勝利の自動化なんてない自動化の不都合な真実自動化も腐りやすい最初はうまくいくOS やミドルウェアのバージョンを上げると死ぬアプリ毎、ホスト毎に個別化、属人化やシステムの複雑化が進行自動化でトラブルので手作業が多くなっていく触るのが怖くなったら秘伝のタレが腐敗している合図腐らないようにする自動化約束されてる勝利の自動化は実は部分的Immutable Infrastructureインフラを管理する手法の一つで「一度構築した本番環境には更新やパッチの提供などの変更を加えず稼働させる」というような考え方自動化を進めていく中で発見された素晴らしいプラクティス常にクリーンインストールから開始必要なものは全て固めアプリを共存させないコンテナへの道アプリに必要なものを全て特定のフォーマットで固めて、展開するだけで起動2013年に最初にリリースされ、Docker,Inc.によって開発Dockerは「コンテナ」と呼ばれるソフトウェアパッケージを実行される起動の約束された勝利のアプリケーションベースイメージをダウンロード必要な各種ソフトウェアは全てコンテナ内にインストール必要な設定はコンテナ作成時に仕込む自動化とコンテナのざっくりとした関係同じモノとして扱いやすくなるインフラの腐敗を防止運用が統一的開発でテストし、そのままを運用に適用可能環境の影響を受けずに自動化の負担は軽減クラウドによるアプリケーションファーストクラウドにより組み上げ方式から呼び出し方式へ要求すれば下位のリソースが自動的に割り当たるコンテナによって取り扱いを共通化アプリとインフラの依存関係を断ち切ることができるCloud NativeとWebサービス基盤モデルについて再考Service Level indicator とService Level Objective信頼性 100%と、信頼性 99.99%では大きな違い信頼性 100%を実現するためには、99.99%とは異なり膨大な工数を投入が必要ほとんどのユーザーにとっては「99.99%」が「100%」になったからといって、大きなメリットがあるわけではありません🥺🥺🥺各サービスごとに適切なSLOを設定することが大事SLOとは、SLIで計測されるサービスレベルの目標値、または目標値の範囲を指しますSLOを「年99.99%」と設定すると、「1年のうち52分は稼働しなくてもよい」ということになりますしかし、シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯService Level Indicator の例リクエストのレイテンシ（リクエストに対するレスポンスを返すまでにかかった時間）エラー率（受信したリクエストを正常に処理できなかった比率）システムスループット（単位時間あたりに処理できるリクエスト数）可用性（サービスが利用できる時間の比率）シングルノード100 台でアプリを動かすと…　ｱｯｱｯｱｯクラウドを使ったからといって…可用性が勝手に高まるわけではないシングルノードでの運用について* アプリの更新は？* サーバー自体が電源断でサービス落よね？* 負荷が増えたらどうするの？* アクセスが増えたらどうするの？何らかの方法でリスクを回避せねば、その方法の一つがKubernetesでありCloud Native となるCloud Native とは？github.comコンテナサービスメッシュマイクロサービスイミューダブルインフラストラクチャおよび宣言型APIなどを用いて回復性管理力可観測性堅牢な自動化によって変化に強い疎結合なシステムを実現する疎結合ではないシステムとは？Availabilityコンポーネントが死ぬと全体が死ぬScalability一つの機能をスケールさせるためには全体のスケールが必要Complexity新しい機能を追加するときに全体との調和が必要なので大変疎結合なシステムとは？Resilient一つのサービスが死んでも一部のサービスは継続Flexible Scaleサービスごとに独立してスケールリソースの最適化Simplicity小さな単位で開発することにより新機能の追加が容易になるKubernetes の特徴不変なインフラ一度、構築したインフラは変更を加えることなく破棄して、新しいものを構築しなおせばよいImplicit or Dynamic Grouping(入れるところないのでココに書いておきます)宣言的設定命令的に手順や変更履歴を記録するのではなく宣言的な設定ではシステムのあるべき姿を定義します。Kubernetesはこの定義ファイルを確認してあるべき姿に自律的に動作するDeclarative Configuration自己修復機能Kubernetesが障害や異常があった時にあるべき姿になる為にシステムが設定した通りにAPIを再起動したり様々な作業を自動で行うReconciliation Loopにて実現Webサービス基盤モデルについてサービス層実際のWebアプリやWebサービスのコンテンツ層ストラテジー層Webサービスの特性に合わせてコンテナ基盤をより特徴的に制御する層オーケストレーション層コンテナ群や収容ホスト群のモニタリングやリソース管理等によってCRIを介してコンテナを制御する層コンテナランタイム層コンテナそのものの制御層インフラストラクチャ層ハードウェアやVM、ベアメタル等のコンテナのリソースプールを実現する層ストラテジー層の進化と拡大istioマイクロサービスアーキテクチャにおけるネットワーク面での課題を解決する機能群を提供するKnativeモダンなサーバーレスワークロードをビルド、デプロイ、管理するためのKubernetesベースのプラットフォーム Knative など、さまざまな用途のアプリが誕生しているストラテジー層の拡大とDapr についてDapr とは効率的なクラウドネイティブアプリ開発を目指した分散アプリケーションランタイムDapr は サイドカーによりサービス間の呼び出し、ステート管理、サービス間メッセージングなどの非機能要件を実現する事で分散アプリケーションの実装上の課題を解決する機能群を提供するフレームワークです。非機能的ではあるが本来、サービス層が持っていた一部機能をストラテジー層が担っている。ComplexityとDapr の実装についてDapr とは？Distributed Application Runtime の略Daprの特徴サイドカーにより任意の開発言語やフレームワークで開発可能ベストプラクティスをビルディングブロックとして提供Goalあらゆる言語やフレームワークを使用して、分散アプリケーションを記述することが可能ベストプラクティスのビルディングブロックを提供することで、マイクロサービスアプリケーションの構築で開発者が直面する困難な問題を解決コミュニティ主導で、オープンかつベンダーニュートラルであること新たな貢献者の獲得オープンAPIによる一貫性とポータビリティの提供クラウドやエッジなど、プラットフォームにとらわれないベンダロックインすることなく、拡張性とプラグイン可能なコンポーネントを提供する高いパフォーマンスと軽量化により、IoTやエッジのシナリオを可能にする実行時に依存することなく、既存のコードからインクリメンタルに採用できるサイドカー パターンとは？分散システムにおけるデザインパターンの一つサイドカーは、アプリケーションコンテナを拡張および拡張して、機能を追加します。サイドカーを使用して既存のレガシーアプリケーションなどにも適用できます。同様に、これらを使用して、一般的な機能の実装を標準化するコンテナを作成することもできます。Dapr におけるサイドカーアーキテクチャDapr の多様性Dapr サイドカーにより、HTTP/gRPC での通信が可能であれば開発ができる公式SDKも提供されているビルディングブロックビルディングブロック一般的にはシステムアーキテクチャを構成する要素Dapr では利用可能な機能群のことを指す場合が多いマイクロサービスのベストプラクティスを体系化して機能として実装されてるサイドカーのHTTP/gRPC を呼び出してこれらを利用することができる2022年2月で 8つのビルディングブロックが用意されているサービス間呼び出し状態管理パブリッシュとサブスクライブバインダーアクター可観測性シークレットの管理構成設定コンポーネントビルディングブロックで利用される機能モジュール一つ以上の複数のコンポーネントを使用可能IF が用意されているのでこれらに合わせて機能を実装統一されたエンドポイントが利用できるのでアプリ側に複雑性を抱え込まなくて良いRepository において Dapr による抽象化の理想と現実Repository とはDDDのレイヤードアーキテクチャで提唱されいるRepository でインターフェースを定義することによりInfra層を抽象化、依存性の逆転モックの差し替えが可能になり、Application層のユニットテストが可能になるDapr によるRepository の吸収しきるのか？何を抽象化しているのか？Repository は、抽象化とレイヤー化を同時に行うのが一般的Dapr は前述したビルディングブロックと公式のSDKによってプロトコルの抽象化が可能抽象化に合わせた実装を一部しなくても良いので全体の実装量はそりゃ、減るチームでどのような実装にしていくか話し合いが必要であり、レイヤー化に関してはあまり寄与しないDapr によって Complexity は解消されるのか？Complexity とは認識や変更を困難にするソフトウェアの構造に関する全てのものを指しますどれだけ実装が「複雑」でも開発者が読み書きする必要がないようになっていれば、それはComplexityとは言いませんProxy が担う部分はまだ、機能がまだ少ないDaprでComplexity の解消の夢は見れるか？Dapr によって抽象化の一部のメリットは得られるDapr でもレイヤー化するのは自分達であることを忘れずにMockClient みたいな話がgo-sdk でも出ればいいが特にないので自分達で用意する必要がある参考資料Dapr Docshttps://docs.dapr.io/Infrastructure as Codeのこれまでとこれから/Infra Study Meetup #1 よりhttps://forkwell.connpass.com/event/171560/ふつうのLinuxプログラミング 第2版　Linuxの仕組みから学べるgccプログラミングの王道https://www.sbcr.jp/product/4797386479/コンテナ時代のWebサービス基盤モデル - FastContainerの研究発表をしてきましたhttps://rand.pepabo.com/article/2017/06/28/iot38-matsumotory/目的に沿ったDocumentation as Codeをいかにして実現していくか / PHPerKaigi 2021https://speakerdeck.com/k1low/phperkaigi-2021クラウドネイティブとKubernetes（だいたいあってるクラウドネイティブ）https://speakerdeck.com/hiro_kamezawa/kuraudoneiteibutokubernetes-daitaiatuterukuraudoneiteibuDesigning Distributed Systems (PUBLISHED BY: O'Reilly Media, Inc.)https://learning.oreilly.com/library/view/designing-distributed-systems/9781491983638/ボトムアップドメイン駆動設計https://nrslib.com/bottomup-ddd/Repositoryによる抽象化の理想と現実/Ideal and reality of abstraction by Repositoryhttps://speakerdeck.com/sonatard/ideal-and-reality-of-abstraction-by-repositoryA Philosophy of Software Designhttps://web.stanford.edu/~ouster/cgi-bin/book.php","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/17/182336","isoDate":"2022-02-17T09:23:36.000Z","dateMiliSeconds":1645089816000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"さよなら、俺のVim。Neovim への旅立ち","contentSnippet":"はじめにこんにちは、nwiizo です。私は今から10年前にVim を使い始めました。Vimはviから発展したテキストエディタです。コード補完、コンパイルまたエラージャンプなど、プログラミングに特化した機能が豊富で、広くプログラマに使用されています。私も思考のスピードでの編集をvim で実践してきた1人でした。2022年の現在ではVim vs Emacsなど言われていた時代も遠い過去になり、VSCode１強になりつつあります。そういう、私も少ない設定で動作する強力で最高のJetBrainsやVSCode に浮気をした回数は数え切れません。IDEの生産性を身に染みて感じながらも、身についた操作感/キーバインド及びターミナルからの起動の速さが辞められず。しかし、vimrc を強力に設定しているわけでもなく愛の力のみで心の擬似IDEとしてのvimを使っておりました。進まねばならぬこのままでは愛に沈む。愛に殉じたいが痛いおじさんになりたくない。意を決してVim からNeoVimへの移行を決めました。ふと、NeoVim に環境を移すことを決めました。— nwiizo (@nwiizo) February 7, 2022  俺のvimrc過去にはこんなことを言ってた。やってること変わってない。syu-m-5151.hatenablog.comgithub.com初期構想をやめましたNeovim + coc.nvim + (Neo)vim Plugin で初期構想を考え手を動かしてましたが、結果として断念しました。理由として、今夜中に変更したかったこと。既存のプラグインに、そんなに力を入れていなかったこと。深夜テンションで入れ替えを行なった為に、下調べが足らずにプラグインの選定や大量に入れたプラグインの起動時間の短縮などがめっ… 難しかったからです。起動時間を短縮しようとしてる様ですNeoVim 感情のままplug-in 入れてそのまま起動すると全然、立ち上がらない。生活を考えてから設定作らないとだめ。— nwiizo (@nwiizo) February 8, 2022  こちらのドキュメントは非常に参考になりました。ありがとうございます。zenn.dev選ばれたのは、よい設定を求めてインターネットをさすらっているとvim-config なるリポジトリに出会いました。欲しかったプラグインがほとんど入っており、何より先ほどまで苦戦していた起動時間が短いという単語に惹かれてすぐに入れて動かしてみました。github.comvim-configがどのようなプラグインや設定を使ってどのように設定を実現させているかやいくつかのショートカットについてはリポジトリで確認お願いします。おそらく、それだけでかなり、勉強できるのでおすすめです。使用感は最高でプラグインのショートカットをいくつか試したりコードを書いたりしました。また、各言語の設定については~/.config/nvim/config/local.plugins.yaml などに設定を入れておくと良いですが。私は、vim-goを入れました(がのちに削除)。:GoDef による定義位置ジャンプは vim-lsp が有効になっていれば gd で可能です(sbで前のbufferに戻れるのでそれで行き来できる。)。公式ドキュメントの設定に従ってImports を設定する。その内にlspの自動補完が上手くいってないことに気付いたので色々設定を見ていき~/.local/share/nvim/ などの共有設定に阻害される設定が入っていたので移動させた結果、無事にlsp 自動補完も動作しました。~/.local/share/nvim/ などの既存設定file が邪魔してないかを確認して.config/nvim/config/local.plugins.yaml にvim-go を入れたらちゃんと動きました— nwiizo (@nwiizo) February 7, 2022  さいごにこれが大好きだったvim との別れです。今日からはこれでコーディングしてみたいと思います。大好きだったvimが強くなり、帰還した。そういう、感じがして今日はとても素敵な気分です。朝会で共有したらVSCodeではないのかって笑いが起きました。ちゃんと前に進んでいるいい職場です。私のnvim の設定をこちらに置いておきます。現在はこちらでいくつかのlspを入れて開発しております。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2022/02/08/130305","isoDate":"2022-02-08T04:03:05.000Z","dateMiliSeconds":1644292985000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『Learning Dapr』を読んだので感想文","contentSnippet":"『Learning Dapr』を読んだ。輪読会ではなく共有会としての読書感想文を記載しました。Dapr に関する素晴らしい書籍だと思うので共有会を通して皆さんに共有したいと思います。誰が読むべきか？Dapr の実装が必要な方分散システムに関して兎に角見聞を広げたい方Learing Dapr についてlearning.oreilly.comどんな本なのか？Daprがステートレス、ステートフル、アクタープログラミングモデルを統合するだけでなく、クラウドやエッジのあらゆる場所で実行される方法を背景からその将来について開発者自ら、説明してくれている書籍。話の主軸がDaprであるためどうしても、Daprを利用する以外では他の書籍を読んだ方が効率が良いというか理解しやすいと思う。分散システムの実装に関して別の観点から理解を深めたければ読めば良いと思うまた、どんな本ではないのか？分散システムを学べる本ではないKubernetes が学べる本ではない主要なコンテナオーケストレーターであるKubernetesを本番環境で動かすために役立つベストプラクティスやKubernetes の概念が学べる本ではない。Production KubernetesGolang に入門できる本ではないGolang の入門的な内容はなく各章ごとに適当な言語で書かれているThe Go Programming Languageマイクロサービスに関する本ではないマイクロサービスのためのシステム分割や設計のレベルから解説する本ではない。あくまでdapr のための書籍であってシステム分割や設計が前提としてマイクロサービスパターン 実践的システムデザインのためのコード解説 - インプレスブックスDapr とはそもそも、Dapr とは、Microsoftが中心になって開発しているOSSの分散アプリケーションランタイム、Distributed Application Runtimeの略でDaprです。Dapr は様々なクラウドサービスやミドルウェアを良い感じに透過的に扱うことを目的としたプロダクトで、なかなか筋が良いのですが、何に使えるかよく分からないというか、そもそもどういうものか分かりづらいので今回はオライリーから出版されたLearning Dapr を年末に読んだので共有会で共有していこうと思います。スター数が15K とかなりいきおいのあるOSSのプロジェクトではないかと思いますDaprの特徴Dapr はサイドカーとして利用することで。本来実装したいコアロジックに集中でき簡単にマイクロサービスを作成することができます。また、デプロイする環境はKubernetes もしくはローカル環境を選ぶことが来ます。更にそれぞれのビルディングブロックは抽象化されており、 HTTP/gRPC API を通して利用するものとなっているため言語に縛られない開発ができるのも魅力となっています。それぞれのコンポーネントはライブラリとしてアプリケーションに組み込むのではなく、yamlのコンポーネント定義ファイルをロードさせることで利用することができるので実装に一切手を加えず、検証環境ではredis、本番環境では何かしらのクラウドサービスなど切り替えが可能。Service-to-service invocation: /v1.0/invoke他のマイクロサービスサービスへ通信するための機能State management: /v1.0/statekey/valueベースの永続化や参照機能Publish and subscribe: /v1.0/publish and /v1.0/subscribePublish/subscribeモデルで非同期にメッセージを送受信する機能Resource bindings: /v1.0/bindings外部コンポーネントやサービスを抽象化しイベントの送受信を行う機能Actors: /v1.0/actors分散性や並行・並列性をもち、非同期なメッセージ駆動のアクターモデルを提供Observabilityログ・トレース・メトリクス・ヘルスチェックといったオブザーバビリティに必要な要素を提供(この辺の情報は整理してObservability Conference 2022にCfPに投げます)Secrets: /v1.0/secrets安全にパスワードなどのクレデンシャルなデータにアクセスする機能Extensible拡張性に優れたミドルウェアRate limit やOAuth2 、Open Policy Agent などさまざまな機能をミドルウェアとして実装可能となっています。  components-contrib/middleware/http at master · dapr/components-contribサポートしてるSDKdaprが提供する HTTP/gRPC API にアクセスするためのsdkを利用することもできます(クライアント同士の直接参照も可能なので)。提供されているSDKは、以下の8つの言語になります。Java-sdkPython-sdkDotnet-sdkJs-sdkGo-sdkCpp-sdkPHP-SDKWIP: Rust-sdk今後、サンプルを書く際にはGo-sdk を利用して書いていくこととします。Dashboard個人的に気に入っている機能としてはダッシュボードの存在があります。Kubernetes に対しても実行できるので非常に重宝をしております。https://github.com/dapr/dashboard/blob/master/docs/development/changelog.mdやっと、目次全7章から成り立っています。どの章も公式ドキュメントよりも背景であったりとかメタ情報が付与されており1. Services2. State3. Messaging4. Security5. Actors6. Application Patterns7. Dapr’s Future1. ServicesDapr 対応アプリケーションの基本的な単位はサービスと呼ばれます。この章では各サービスの状態管理やトレース、必要なときに安全な通信様々な機能がどのように使えるのかを簡単に説明した章になります。この章までであれば公式ドキュメントを読めば大体なんとかなる2. StateDapr の状態管理は、プラットフォームに依存しないクリーンな状態処理コードを記述しながら、これらの課題に対処するのに役立つ単純な状態APIを提供することを目的としています。状態管理についてクラウド化や様々なデータストアにどのように対応していくかどういった方法が取れるかなどが記載がされております。3. MessagingDapr は pub/sub にKafka やRabbitMQなどをバインディングすることが可能で一般的なメッセージング構造を提供しますが、独自のメッセージングバックボーンを作成しません。Dapr の pub/subを使用すると、メッセージパブリッシャーはトピックにメッセージをパブリッシュでき、トピックのすべてのサブスクライバーはメッセージのコピーを取得できます。Daprは、メッセージが少なくとも1回処理されることを保証します。上記で説明したようにDaprはpub/sub に外部リソースのバインディングが可能でそれらをもとにシステムを作成することができます。この章ではそういったdaprの機能や特性を活かしてどのようなアーキテクチャが考えられるかについて書かれた章4. SecurityDaprは追加の機能としてではなくデフォルトで提供します。それらがどのように実装されているかという記載はないですがどうやって実装していくかについて記述がされている。Daprは、シークレット管理、シークレットAPI、相互TLSサポートなどの基本的なセキュリティ機能のセットを提供しています。いくつかの追加のセキュリティ機能のIssueや が挙げれらている。5. ActorsDaprは、クラウドネイティブで復元力のあるプラットフォームに依存しない仮想Actorモデルを提供します。ActorランタイムはDaprランタイム内で実行されるため、Dapr上に言語固有のアクターSDKを簡単に記述でき、他の機能と同様にHTTPまたはgRPCを介して任意の言語からアクターを呼び出すことができます。この章ではActor によるターンベースの同時実行、状態管理、Timer やreminderなどの独自の機能を説明している。Daprがアクターインスタンスを独立したプロセスとしてではなく、同じWebサービス上のルーティングルールとして扱うことです。これにより、Daprは高密度でアクターインスタンスをホストできます。6. Application Patternsdaprのアプリケーションコードは、さまざまなイベントソースからのイベントに応答し、コネクタを介して他のシステムにイベントを送信/受信できます。クラウド環境でどのように利用できるのか？既存のサービスメッシュとどのように協業するのかについて記載がされている章単発で為になる記載もいくつかあったが何回読んでも難しすぎて参考文献になりそうなものを読んだりして行ったり来たりしてる7. Dapr’s Futureシェルスクリプトとしてのdapr の実行やWebAssembly,アクターの各種機能などDapr が将来どのように成長するかを開発者自身が書いてくれている章","link":"https://syu-m-5151.hatenablog.com/entry/2022/01/18/124731","isoDate":"2022-01-18T03:47:31.000Z","dateMiliSeconds":1642477651000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"2021年9月 の労働などの振り返り","contentSnippet":"登壇理論と実践からSREを再考するSRE Gaps「理論と実践からSREを再考する」で座談会のファシリテーションを行いました。過去に翻訳刊行された『SRE サイトリライアビリティエンジニアリング』『サイトリライアビリティワークブック』を読んで、SREに取り組む企業が増加しました。がこれらの本で言及されている事例はほとんどがGoogleの事例でした。今月『SREの探求』という本が出ました。この本は、Google以外の会社がSREを導入実践する内容がメインになっております。本のリリースの記念したイベントでもあります。また、本の内容については今後、文章にしていきたいと思ってます(第1章がコンテキストに関する内容でまとめるのめちゃくちゃ難しい)。forkwell.connpass.comsyu-m-5151.hatenablog.com来月は所属組織であるスリーシェイクのイベントでGo言語の可観測性に関する内容で登壇しようと思います。3-shake.connpass.com現状では本を書いてたり書くための技術検証を行なっているので登壇資料に落とし込むかーっと思うなどしてます。次の技術書展は本を出したいと思っているのですが何からはじめればいいんですかね？— nwiizo (@nwiizo) 2021年7月8日  3-shake.com での仕事2021年6月1日に転職して丸4月経過しました。とても、学びと悪戦苦闘の日々を送っており、所属としてはSreake事業部になります。私たちはSREのプロフェッショナルパートナーですSreake（スリーク）は、金融・医療・動画配信・AI・ゲームなど、技術力が求められる領域で豊富な経験を持つSREの専門家が集まったチームです。戦略策定から設計・構築・運用、SaaS提供まで、幅広い領域をサポートします。sreake.com現職では支援事業を行なっており、さまざまなプロジェクトに参加しておりますが、これらが喋っていい内容かどうかは不明なので詳しい技術に関する話はしません。SRE支援事業の感想SREの支援事業をしていて思うことはSREの導入に成功している企業は、Googleが提唱しているSREの概念を理解しつつ、自社の状況やプロダクトのフェーズにあわせて常にSREのあるべき姿を変化させていることです。「SREはこうでなくてはいけない」という固定概念にとらわれず、常に柔軟に変化する意識を持つと良いのではないかと思っている。 変化するものだけが勝つとか誰も言ったことないようなこと言いたいと思います。SREはDevOpsというinterfaceの実装であるまた、SREを実装するうえで技術的な要素はもちろん大切ですが、SREの組織として「どのようにすればうまく状況が共有されるのか」「どのようにすれば横断的な運用管理ができるのか」を考えて行動することも大切ではないかと思います。技術のみに囚われすぎず、定期的なミーティングや組織体制の見直しを行うことで、より良い体制作りができるのではないかと夢想してました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/10/01/171059","isoDate":"2021-10-01T08:10:59.000Z","dateMiliSeconds":1633075859000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「理論と実践からSREを再考する」というイベントで司会進行とパネルディスカッションのモデレーターをやる #SREGaps","contentSnippet":"はじめに本日、2021年9月9日19時30分より『SRE Gaps「理論と実践からSREを再考する」』というイベントがあります。私、nwiizoはそこでパネルディスカッションのモデレーターを行うことになりました。forkwell.connpass.com500名以上も参加登録されていてかなり注目されていると思います。SRE という概念を導入する日本企業の開発組織も増えてSREの考え方は多くの場所で取り入れられ、実践されています。 しかし、SRE の実践方法はプロダクトの性質や組織の規模、チーム構成によって異なるほか、 文化としても各企業それぞれ異なる解釈をしている場合が多く存在しており有効に活用されていない現状が見られます。とりあえず、流行っているので「インフラエンジニア」という役職名だけ「SRE」にしており実務としては変わらないままみたいなことが聞こえてきて悲しいです。SREはyaml 管理者のことを指していうわけではないです。ここでインフラエンジニアとSREの違いについて三つ紹介したいと思います。1.業務範囲1つ目は、インフラエンジニアは「インフラのみ」が業務範囲であるのに対して、SREは「信頼性を高める活動全て」が業務範囲である点です。インフラエンジニアは、アプリケーション開発チームが開発したサービスが、「高いパフォーマンスで安定的に稼働する」ための環境を構築し運用することが役割です。よって、インフラの構築・運用・改善は行いますが、アプリケーション側には責任は持ちません。これに対してSREは、「サービスの信頼性を高めるための全ての活動」を行います。具体的には、インフラだけでなくアプリケーション側も業務範囲となるため、アプリケーションのプログラムの修正までSREチームにより行われる場合もあります。また、開発や運用のみならず、組織や文化の醸成といった部分まで責任を持って取り組まれる例が多いのも特徴です。2.スキルセット2つ目は、業務範囲の違いからくる、求められるスキルの違いです。インフラエンジニアは「ITインフラに関する知識や技術力」が求められますが、SREはインフラエンジニアが持つ知見に加えて「アプリケーション開発を行う技術力」や「当該アプリケーションに関する深い知見」が求めれます。求められるスキルの違いは、SREチームの構成にも反映されています。SREを提唱したGoogle は、「SREチームの約半分はGoogle の正規のエンジニアで構成される」としています。つまり、SREチームの半数はアプリケーションエンジニア（または経験者）により構成されるべきとしています。そして残りの半数は「Google の正規エンジニア『予備軍』だが、他のメンバーが持っていないスキルがある」ことを条件としています。ここで指す他のメンバーが持っていない具体的なスキルとしては、「UNIXシステムの内部構造」と「ネットワーク（レイヤー1からレイヤー3）」の専門知識であることが圧倒的に多いです。この辺は2011年に発売されたウェブオペレーション――サイト運用管理の実践テクニックなどを参考にしていただけると非常に参考になると思います。言い換えると、SREチームメンバーの半数は、SREからアプリケーション開発チームに異動しても、そのまま業務を行えるレベルで開発力があるメンバー（または直前までアプリケーションチームに所属していたメンバー）で構築されるべきだということです。一般的なインフラエンジニアの多くは、アプリケーション開発チームに異動したとしても、スキルがマッチしないため、その業務遂行が難しいことを考えると、SREチームは技術力の「深さ」だけでなく「広さ」も求められるといえます。3.方法論3つ目は、方法論の有無です。「インフラエンジニアとは、なにをどのようにして行うべきか」という方法論は、企業により大きく異なります。これに対して、SREは明確な方法論があります。具体的には、上記で紹介したGoogle が自社のSREの紹介サイトhttps://sre.goole/において、『Site Reliability Engineering』という「SREの原典」ともいうべき本を無償で公開しています。これらには日本語版や他の言語のものもあるので全世界のSREは、この「SREの原典」を理解した上で、記載されている方法論に従ってSREの業務を行っています。もちろん、企業ごとに「どこまで原典を文字通りに取り入れてSREを行うか」の違いはありますが、大まかな方法論や用語、考え方は全ての企業で共通しています。vs DevOps というおまけWebサービスの信頼性や価値の向上に用いられるアプローチ方法としてSRE（Site Reliability Engineering）というものがあります。システム開発側と運用側の溝を埋めるために生まれたこの手法ですが、従来のDevOpsとはどのような違いがあるのでしょうか。ついでにSREとDevOpsの違いについて見ていきます。SREとDevOpsの違いや関係性を知るには、Googleが提唱している「class SRE implements DevOps」の考えが最も明解でしょう。「class SRE implements DevOps」は、「SREはDevOpsというinterfaceの実装である」という意味を表します。「DevOps = 思想」という定義に対し、それを具体化し実装したものがSREであるという考えです。この辺は概念的な面も多く「実際、どのようにSREを導入すれば良いのだろう？」や「専任のSREチームなしでSRE原則を適用する方法がない」と思う担当者の方も多いかと思いますのでぜひ、紹介した本を読んでみましょう！さいごに現在、以下の3冊がGoogleから出されています。Site Reliability EngineeringThe Site Reliability WorkbookBuilding Secure & Reliable Systemsこのうち、Site Reliability Engineering と The Site Reliability Workbook は日本語版も出版されております。sre.googleそして、日本語での新たなSRE関連書籍が9月3日に発売されました。ちなみにGoogle からではないです。この書籍は大規模なプロダクションシステムの運用において、様々な企業や組織がSREをどのように実践しているかについて紹介している書籍になります。その本のタイトルは『SREの探求――様々な企業におけるサイトリライアビリティエンジニアリングの導入と実践』 です。私は6月にSRE特化型コンサルティング事業を運営するスリーシェイク社に転職して1ヶ月程無職期間を謳歌していたので他にもSRE関連書籍を読みましたがその中でも今回のイベントのタイトルでもある理論と実践について深く書かれているので原典を読んだ上で読むとめちゃくちゃ面白い書籍だと思います。イベントに参加できなくとも信頼性に関わる全てのエンジニアは読んでも良いと思いました。本イベントでインフラエンジニアからSREと名付けられ旅館で迷子になった無垢な子供が救われることを祈ってます。それでは皆様、イベントでお会いしましょう！また、株式会社スリーシェイクではSRE に関するイベントをやっております。登壇者含めて募集しているので皆さん登録よろしくお願いします。3-shake.connpass.comあとがきSRE Gaps「理論と実践からSREを再考する」は本当に良い発表ばかりだったと思う。自分は本当に何もできずにただただ震えてただけですがなんとかいいイベントになったのではないでしょうか？皆様も感想などありましたらハッシュタグ付けてツイートでもしてください！twitter.com2021/9/9 「SRE Gaps 理論と実践からSREを再考する」イベントリポートsreake.com完全なる宣伝になるんですけど「実際、どのようにSREを導入すれば良いのだろう？」や「専任のSREチームなしでSRE原則を適用する方法がない」と思う担当者の方も多いかと思います。弊社は、金融・医療・動画配信・AI・ゲームなど、特に技術力が求められる領域で豊富な経験を持つSREの専門家が集まったチームです。戦略策定から設計・構築・運用、SaaS提供までSREに必要な要素を統合的に提供可能です。もし少しでもSREに興味があるという企業様がいらっしゃいましたら、気軽にお問い合わせください。sreake.com","link":"https://syu-m-5151.hatenablog.com/entry/2021/09/09/142150","isoDate":"2021-09-09T05:21:50.000Z","dateMiliSeconds":1631164910000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『SREの車窓から』という題で #geekgig に登壇。 ","contentSnippet":"前書き『GeekGig #1 〜Goと私の一年〜』という株式会社Showcase Gigが主催しているイベントでLTで登壇してきました。運営の皆様ありがとうございました。今回の登壇はSREというよりもシステム管理者がGo言語を使うメリットについて言及しました。タイトルはミスリードです。申し訳ございません。しかし、SREやシスアドの仕事は本当に楽しいことばかりではないですが如何に楽しくやっていくかを考えるのは楽しいなと思っています。showcase-gig.connpass.comまた、Modern System Administrationでは特定の言語に縛られてはないのですがシステム管理者がプログラミング言語を習得することについてのいくつかの言及があったので読んでみてください。learning.oreilly.comSREやインフラエンジニアではないのですが個人的には小野 和俊さんの書籍である「その仕事、全部やめてみよう」で言及されている【ラストマン戦略】が好きで、コードが読めたり書けたりするインフラマンというのは重宝されるように思えます。今回の発表はその啓蒙の一つだと思います。www.bookbang.jpインフラとGo言語が混ざったFukuoka.go という最高の勉強会があって直近のものに関しては動画も公開されているのでみておくのも良いかと思います。fukuokago.connpass.comまた、勉強会やコミュニティーに対して定期的に参加することは技術的なモチベーションにも繋がるので発表者になるのも良いかと思います。nulab.com登壇資料の文字起こしと補足各分野や文字に対して響いた時に読んでほしい本がいくつかあるので記載しておきます。 speakerdeck.comQ.今年は私がGo言語で何をしていたか？A.ほぼ全てで Go言語を利用している生業としてはインフラやプラットフォームの構築や開発をしたり、インフラでの便利ツールの作成をやってました。一昨年ぐらいから規模感でPythonやシェルスクリプトを選択するという場面が減りました。私は作業シェルにfishを利用しているのですがこれらの環境に直接影響があるものや自分でライブラリーの開発が必要でそれらが面白くなさそうな場合以外は全てGo言語で実装するようになりました。最近では、個人開発の生産性の観点からも慣れのおかげでGo言語一択になっています。自動化楽したいあわよくば全ての作業は自動化されてほしい。しかし、開発スキルなしでは正確に自動化できず、運用スキルなしでは正しく自動化できません。昨今の運用が抱える自動化はソフトウェアチームの核心である継続的なデプロイや義務である継続的デリバリーだけではありません。しかも、この分野に全ての問題を一挙に解決してくれるカリスマ的なツールは存在せず。いくつかのツールを合わせて利用することになり、Go言語はさまざまなツールに対応しております(ちなみに、シェルスクリプトの知識は大事)。自作可能。クラウドネイティブな世界ではプログラマブルに制御できる範囲がGo言語だと広い関連書籍learning.oreilly.comlearning.oreilly.comlearning.oreilly.comlearning.oreilly.comコラボレーション開発者には運用スキルが必要です。彼らの責任はコードを書くことだけではなく、システムを本番環境にデプロイしてアラートを監視することです。同様に運用者にも開発のスキルが必要です。本番アラートを監視することだけではなく、不具合が起こった時の事象の認識、コードの特定、変更などできれば良い関係になるんではないでしょうか？もし、開発チームと運用チームが同じプログラミング言語で開発していたら素晴らしいと思いませんか(まぁどんな言語でもいいけど)？。それこそ、書籍でしか文字として認識しかしてない開発チームと運用チーム間の双方向のコラボレーションが発揮させれるのではないでしょうか？開発と運用が協力してツールや知識は開発と運用の間のすべてを双方向に生かされて成功は約束された感じがしてきませんか？最近だと、Node.jsやDartなどもっと複数のレイヤーを跨いで協業できる言語も増えています。各チームや組織ごとに最適なものをその場、その場で決めていけば良いと思います。10x.co.jp関連書籍learning.oreilly.comlearning.oreilly.com解像度Goで開発された世界の素晴らしいツールやミドルウェアの実装が読めると自分のチームの開発や運用の役に立つことや自分自身の実力になることも多く良い循環がまわるようになる。現在、SREやインフラエンジニアが使う多くのツールやミドルウェアがGo言語で開発されている。そのため、Go言語特有のエラーやログのメッセージに慣れておくことによって今まで、不思議で意味のなかった文字列が開発者からのメッセージに見えてくるようになって、デバッグ時や問題発生時に非常に役に立ちました。ツールやミドルウェアを使っている時の解像度がグッと上がった気がします。関連書籍learning.oreilly.comlearning.oreilly.com最後にこの発表はハッカー的な意味合いというより自戒や啓蒙などを込めたものでした。年に何回かエモいだけの発表がしたくなるのですが相応に数人が楽しく仕事できればもはやなんでも良いのではないかと思いだしました。次回があればもっと技術に寄った発表をします。でも、転職して3ヶ月で思ったことは本当にhttps://dart.dev/","link":"https://syu-m-5151.hatenablog.com/entry/2021/08/17/114732","isoDate":"2021-08-17T02:47:32.000Z","dateMiliSeconds":1629168452000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"『よみがえるMakefile』という祈り、もしくはJuly Tech Festa 2021 登壇感想","contentSnippet":"概要7月18日に July Tech Festa 2021 が開催されて『よみがえるMakefile』 というタイトルで15:20 から登壇した。今さら聞けないIT技術 というお題だったので今さら聞けないけど… みたいなものが議論の種になればとお題をMakefileに決めました。docs.google.comconnpass はこちらtechfesta.connpass.com文章として大きく離散してるので雑多なまとめになります。タイムアポン完全に終わっていて一連のスライドからGo言語のMakefileを育てていくエピソードを登壇資料から抹消した。正直、資料の中にあったものをシュッとMakefileに落とし込むだけなので特定の言語やツールに依存するかな？と思って排除してしまいました。何故か、40分だと思ってたので20分資料に再編集した。— nwiizo (@nwiizo) 2021年7月18日  資料は、『よみがえるMakefile』 という完全にタイトルで勝ちが確定しているタイトルで発表しました。個人的にはMakefile はCI/CD時代においても最強の可搬性を持つ汎用有能ツールであると思っており、Go言語の利用が広がると共に復活したのかな〜って思うなどしてます。 speakerdeck.comふとした、下書き社内での発表の公開用、かつ発表しながらだったので文章としてどこか雑。インフラエンジニアの作業環境はカオスインフラエンジニアの作業環境はカオスである場合が多い。そのような環境に対応する場合にいくつかのツールが存在する雑な環境を構築するときには自作Shellを整備する  ShellScriptで環境構築を自動化 - Qiita冪等な環境構築を行うときにはAnsibleクラウドに環境を構築するときにはTerraformVMを提供するときにはVagrantコンテナで実施する時はDocker上記のような様々なツールを使うことが多い。上記のツールは環境を構築できるがこれら自体を開発していく上での知見はリポジトリには溜まってなかったりする(どのリンターを使うか？確認方法についてなど)。実際に開発や運用の作業する時には、環境構築だけではないやろ。普通に考えて！運用や開発で得たいくつかのTipsを知見として保存及び継承するために選べる手段として1番最初にパッと出来てしまうのが自作のシェルスクリプトである。自作のシェルスクリプトは応用が効いくが具体的に何をするか一つ目の引数が何で二つ目はあれでみたいな感じで総じて不明瞭であり、それらは把握、保守する人間が必要で、自作のシェルスクリプトはよく古のオーパーツとして発掘されてしまう場合が多い。アーメン🙏🙏🙏Makefile を使う理由としては開発には様々な理由がある。ビルドオプションを指定するのに都合がよく、バージョン情報などを埋め込んだりしやすい。事前にコードジェネレータで書き出す部分がある時はそれらに伴ったコマンドを補完できる。また、特殊に凝ったことも出来ないので古のオーパーツに比べて読める気がする特にGo言語でプロダクトを作る時、Makefile を使ってビルドやツールを指定することが多いです(軽量プログラミング言語では使う機会がグッと少なかった。 )Makefileはたまごっちほど容易く死ぬ。その中でMakefile を育てていけると良い、現場の人が永遠に居てくれていつでも対応できるのがいいけど余裕が無かったり、秘伝のタレ化されてて言語化が難しかったりみたいな事態は発生するよなーって思うなどしてます。この辺の知見の中心になれる会社やエンジニアになりてぇ良い資料たちGNU Make 第3版日本語書籍としてGNU Make 第3版が存在している 正直、文法や意味はこれが最強。アンチパターンについてなど全面同意なので読んで！無料だし！オライリーは最強の書籍ですがそれらをはじめる動機にはなりにくいのかな？って思ってるので俺の登壇にも意味があったはず！kubebuilder で用いられるMakefileは参考になるのでK8sを開発で利用するエンジニアは参考になるのでぜひ！目的に沿ったDocumentation as Codeをいかにして実現していくか / PHPerKaigi 2021Learning GoGo で使う Makefile の育て方Go generate最後にこのブログを共有しながらGistでも貼ってくれた方のものに関しては全て転記しようと思うので皆様何卒よろしくお願いします！個人的に今回の登壇は面白かった。要員としては登壇後の懇親会の gather.town でのフィードバックやごちゃごちゃ感、視聴されてる時の一体感があったような気がします。これらはひとえに運営様の努力だと思いました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/07/26/105940","isoDate":"2021-07-26T01:59:40.000Z","dateMiliSeconds":1627264780000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Go言語のCLI作成ツールであるcobra のシェル補完の生成があまりにも簡単","contentSnippet":"概要Goでコマンドラインツールを作成する場合の選択肢としてspf13/cobraがあると思いますが、遂にv1.2.0 がリリースされました👏👏👏。大きな機能追加として completion が追加され、大枠の使い方とシェル補完の生成についてこのブログではやっていくメモ書きになってます。それ以上に言及しようと思ったのですが完全に体力不足でーす。やっていくinstallCLIがあるので利用するさまざまな日本語での入門記事がcobra には存在するので詳しくはその辺を参照してください。ちなみに、公式ドキュメントが最高で公式ドキュメントに誤りがあればコントリビューションのチャンス。$ go get -u github.com/spf13/cobraって思ったら雑な備忘録を自分も書いてましたが一切の参考にならんsyu-m-5151.hatenablog.cominitコマンドラインツールの初期化を行う$ cobra --viper=false init --pkg-name github.com/nwiizo/workspace_2021/blog/cobra_generating_shell_completionsmain.go -> cmd.Execute() -> cmd/root.go の rootCmd.Execute() の順番で実行されるのでそれらに準ずるように実装していくのですがその辺も他の最高の入門記事があると思うので参照してください。package mainimport \"github.com/nwiizo/workspace_2021/blog/cobra_generating_shell_completions/cmd\"func main() {    cmd.Execute()実際に実装する際にはcobra/user_guide.md at master · spf13/cobra · GitHubをやっていきましょうadd fizzbuzzサブコマンドの追加をしたい時にはcobra add を実行していく。$ cobra add fizzbuzzfizzbuzz created at /*****/workspace_2021/blog/cobra_generating_shell_completionsサブコマンドが実装されました。$ go run main.go fizzbuzzfizzbuzz calledcmd/fizzbuzz.go が追加されるのでこちらに実装を追加していけば良い。init()->rootCmd.AddCommand(fizzbuzzCmd) は自動生成されるのでfizzbuzzCmdの中身を改修すれば実装できる。そして、実装したのが下記になる。package cmdimport (    \"fmt\"    \"strconv\"    \"github.com/spf13/cobra\")func fizzbuzz(max int) {    for i := 0; i <= max; i++ {        fizz := i%3 == 0        buzz := i%5 == 0        switch {        case fizz && buzz:            fmt.Println(\"fizzbuzz\")        case fizz && !buzz:            fmt.Println(\"fizz\")        case !fizz && buzz:            fmt.Println(\"buzz\")        default:            fmt.Println(i)        }    }}// fizzbuzzCmd represents the fizzbuzz commandvar fizzbuzzCmd = &cobra.Command{    Use:   \"fizzbuzz [int]\",    Short: \"return Fizzbuzz\",    Long: `return Fizzbuzzreturn Fizzbuzz There is no particular reason because it is a suitable sample`,    Run: func(cmd *cobra.Command, args []string) {        fmt.Println(\"fizzbuzz called\")        var m int        m, _ = strconv.Atoi(args[0])        fizzbuzz(m)    },}func init() {    rootCmd.AddCommand(fizzbuzzCmd)}補完のためにcompletion fish を行う補完するためにはgo run main.go で実行するわけにはいかないのでビルドを行う。go build .rootCmd.CompletionOptions.*** 以下に設定を入れれば設定の変更を行うことができます。デフォルトではcompletionは有効なのでfish に読み込ませて、実際にコマンドを実行。$ ./cobra_generating_shell_completions completion fish | source$ ./cobra_generating_shell_completions [tab]completion  (generate the autocompletion script for the specified shell)  fizzbuzz  (return Fizzbuzz)  help  (Help about any command)一瞬で補完させることができ、あまりの素晴らしさに膝から崩れ落ちた。補完させないようにしたり、その他のもろもろに関してはcobra/shell_completions.md at master · spf13/cobra · GitHubを読んでいけば実装できると思う。また、これらの補完と配置はMakefileに書いておけば良いかなって思いましたが、7月18日の JTF2021に登壇するからそう思うだけなのか。。。docs.google.com最後にcobraは入門記事が溢れているがツールとして日々アップデートされているので公式を見るのがいいなぁと自分の過去の記事を読みながら思いました。リリースノートに記載があると思うが変更点や実装などはこの辺を読むと良いのでぜひに〜github.com","link":"https://syu-m-5151.hatenablog.com/entry/2021/07/05/103447","isoDate":"2021-07-05T01:34:47.000Z","dateMiliSeconds":1625448887000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"「Cloud Native Go」の読書感想文","contentSnippet":"近況2021年6月1日に転職したのですが現職では週2回のエンジニアの共有会があるので、そちらで共有する資料を作ったのですがついでに自分のブログにもポストしておこうとおもいます。あと、転職してから周りも優秀でよく褒められるしいい会社に転職できたと思います。syu-m-5151.hatenablog.comここから下が共有会の資料。Cloud Native Goを読んだ。輪読会ではなく共有会としての読書感想文を記載しました。本当に素晴らしい書籍だと思うので共有会を通して皆さんに共有したいと思います。本書にはインフラや実行環境の変化に合わせてアプリケーションにはどのような変化が必要なのか？についてと偉大なるGo言語に関しての言及がされております。嬉しいですね。Cloud Native Golearning.oreilly.comcloud-native-go/examplesgithub.com誰が読むべきか？WebアプリケーションエンジニアDevOps スペシャリストSite Reliability Engineerどんな本なのか？アプリケーションの設計、構築、および展開の方法は変化しています。アプリケーションレイヤーの変化に合わせてどのようなインフラの概念に精通すれば良いのかの指針になるのではないかと思う。特にスリーシェイクは、モダンなインフラレイヤーの技術力を強みにしているので読んで損はないと思う。また、どんな本ではないのか？Kubernetes が学べる本ではない主要なコンテナオーケストレーターであるKubernetesを本番環境で動かすために役立つベストプラクティスやKubernetes の概念が学べる本ではない。Production Kuberneteslearning.oreilly.comKubernetes で必要なYAMLが学べる本ではないGoのアプリケーションをKubernetes に載せるためなどの具体的な課題に対して必要なYAMLや概念を学べる本ではない。Kubernetes Patternslearning.oreilly.comKubernetes を拡張する本ではないKubernetes の機能をGo言語を利用して拡張して課題を解決したいクラスタ運用者や気鋭なアプリケーション開発者のための本ではない。Programming Kuberneteslearning.oreilly.comGolang に入門できる本ではない3章では入門書籍っぽいことが書かれているが入門書としては全然足りない。これを読んでGo言語かける人間はかなり強いと思う。Go言語入門とかみんなのGoとかで勉強しましょう。The Go Programming Languagelearning.oreilly.com改訂2版 みんなのGo言語gihyo.jpマイクロサービスに関する本ではないマイクロサービスのためのシステム分割や設計のレベルから解説する本ではない。マイクロサービスパターン 実践的システムデザインのためのコード解説 - インプレスブックスbook.impress.co.jp各章ざっくりまとめPart I. Going Cloud Native1. What Is a “Cloud Native” Application? (19:33 mins)コンピューティングの歴史とクラウドネイティブなアプリケーションに関する言及2. Why Go Rules the Cloud Native World (19:33 mins)Go言語がいかにクラウドネイティブな価値観において素晴らしい言語なのかについて熱弁していて良いII. Cloud Native Go Constructs (01:09 mins)3. Go Language Foundations (54:03 mins)ざっくり、Go言語に関する入門(本にも限界があるので！)4. Cloud Native Patterns (52:54 mins)1番、最初にみんな大好きな分散コンピューティングの落とし穴に関する言及があるのですが本書には「Services are reliable: services that you depend on can fail at any time」という一文も追加されております。Goでの分散コンピューティングの落とし穴をアプリケーション層で回避する為のパターンに関する言及がある。Context の言及や使い方に関しては入門としても非常に分かりやすいので読んでみてほしいです。5. Building a Cloud Native Service (87:24 mins)net/httpやgorilla/muxを用いてRESTfulな簡単なKey-value アプリの設計、実装する。そして、いろんなパターンや想定をもとに再実行やサーキットブレイカーの実装などを変更追加していってセキュリティとDocker化をやっていく的なストーリーIII. The Cloud Native Attributes (01:09 mins)6. It’s All About Dependability (39:06 mins)信頼性に関する考察や言及やマインドがまとめられた章。実際の実装はない。Twelve-Factor Appの言及があるがそれに基づいた実装の記載などはない。この章は口で言うのは簡単ですが理解するのと実装と運用していくのがめちゃくちゃ難しい。SREの骨子のような章。7. Scalability (46:00 mins)スケーリングにおける様々な条件と要件、スケールアウトが多くの場合で最良の長期戦略であることに関する言及をしました。状態の有無、アプリケーションの状態が本質的に「アンチスケーラビリティ」である理由について。また、様々な実装について効率的なメモリ内キャッシュとメモリリークを回避するためのライブラリなどの紹介を行いモノリシック、マイクロサービス、サーバーレスアーキテクチャなどの紹介について。8. Loose Coupling (66:42 mins)各コンポーネントが密に結合されていることを確認できる方法と、それらの密結合された各コンポーネントを管理する方法についての言及及びメリットデメリットに関する実装を交えた言及。これ、本物の地獄さ混沌さがサンプルだと足りない。9. Resilience (57:30 mins)復元力や回復力に言及している章。インフラエンジニア的にはヘルスチェックのエンドポイントだけやっていれば良いかもしれないですが連鎖的な障害を事前に検証するための実装や検証方法に関して言及する章。実際に最高のSREサービスを目指すなら確実に習得が必要でかなり、いかつい章。10. Manageability (59:48 mins)どのようにしてソフトウェアの保守性を上げるかの問題。前職でもめちゃくちゃ悩んでいた。Go言語で一つ以上のアプリケーションを書いた場合にはしっくりくると思う。実際の実装まで言及できる能力があると良いと思いました(知らんけど)。参照サイトのこれとか分かりやすかった。Manageability（管理性）とは？www.ni.com11. Observability (92:00 mins)可観測性に関する言及をする時にインフラエンジニアにとってログとメトリクスは馴染み深いですがトレーシングに関しては正直、馴染みがありません(ほんまか？)。この章では可観測性についてのおおまかな概要と各種三本柱の実装についてまとめられてます。トレーシングは概要ですら本当にためになるのでぜひ、読んでほしいです。「データは情報ではなく、情報は知識ではなく、知識は理解ではなく、理解は知恵ではありません。」という最初の一文が良すぎます。知恵を共有できる人間になりたいです。このエントリー以降キャッチアップできていないので情報をキャッチアップして知恵にできるOpenTelemetryについての現状まとめ （2020年6月版） - YAMAGUCHI::weblogymotongpoo.hatenablog.comちなみに共有会は明日。輪読会ではなく共有会での資料になります。関心ごとをもっとフォーカスして良いと思います。","link":"https://syu-m-5151.hatenablog.com/entry/2021/06/16/220322","isoDate":"2021-06-16T13:03:22.000Z","dateMiliSeconds":1623848602000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Golang で正規表現を用いて雑にURLを取得する","contentSnippet":"退職エントリーが予想以上に伸びたので健全な技術ブログを保つため雑な備忘録貼っておきます。敷居はなるべく低くブログを更新を続けるために昼休憩的な雰囲気だけ持ってやっていきます。syu-m-5151.hatenablog.comコード普通に実行すれば値を取得することができます。package mainimport (    \"fmt\"    \"io/ioutil\"    \"net/http\"    \"regexp\")func main() {    // url の指定    url := \"https://3-shake.com/\"    // 正規表現の作成    re, err := regexp.Compile(\"http(.*)://(.*)\")    if err != nil {        return    }    // net/http でのリクエストの発射    resp, _ := http.Get(url)    defer resp.Body.Close()    // []byte でリクエストの中身を取得    byteArray, _ := ioutil.ReadAll(resp.Body)    // 正規表現にあったものを全てlinks に入れる    links := re.FindAllString(string(byteArray), -1)    for i := 0; i < len(links); i++ {        fmt.Println(links[i])    }}実行雑に取得できた。ここから純粋なURLを取得するのは適当に置換してあげれば良いと思います。mac でのコピペはpbcopy が便利なことを知ったので非常に楽です。https://3-shake.com/wp-content/themes/3-shake/assets/images/favicon/favicon.ico\">https://3-shake.com/wp-content/themes/3-shake/assets/images/favicon/favicon_180x180.png\">https://fonts.googleapis.com/css?family=Open+Sans:400,600,700&amp;display=swap\" rel=\"stylesheet\">https://common.3-shake.com/assets/css/3-shake_icons.css\" rel=\"stylesheet\">~~~","link":"https://syu-m-5151.hatenablog.com/entry/2021/06/08/110251","isoDate":"2021-06-08T02:02:51.000Z","dateMiliSeconds":1623117771000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"4年勤めたGMOインターネット株式会社を退職します。","contentSnippet":"はじめに2017年度の新卒で入社したGMOインターネット株式会社を4年2ヶ月で退職いたしました。 様々な人にお世話になったこと自分自身の成長に繋がったことを考えれば本来であれば直接ご挨拶をすべきところ、このような形で失礼致します。しかし、普通に入社できて働けて良かったと思える会社でした。報告勿体ぶるのもどうかと思うので言及しておくと次の職場は既に決まっており、2021年6月1日から、コンテナネイティブに特化したSRE支援事業、データ統合プラットフォーム事業、広告プラットフォーム事業を展開している株式会社スリーシェイクにてソフトウェアエンジニアとして働くことになっております。3-shake.com退職エントリーに書かれていることが本当に真実かは証明できませんし、主観で僕自身が振り返りをしているだけなので暇な方はお付き合い頂き読んでいただければと思います。振り返り人生を振り返るには今しかないので記載してます。会社で何をしていたのか？社内ではGMOインターネット株式会社でのホスティングサービスの開発と運用、お名前.comやいくつかの商材サイトが載っている社内コンテナ基盤の開発と運用、エバンジェリスト業務を行なっていました。いくつかの事例に関しては登壇資料や発表時に言及してましたのでどうぞ。speakerdeck.com入社までバックエンドエンジニアとしてアルバイトをしていたので、就活については、「プログラミング」がたくさんできる会社に入社したいとふわっと考えてました。GMOインターネットに就職することに決めました。入社直後虚無な社会人研修を終えて、私は社内にある多種多様なインフラ全般を扱う部署への配属となりました。入社前はOpenStack を開発する部署への配属を希望していましたが研修やOJT中にCIやIaCがないことに対してイキった発言をしたり、SNSでの一部の言動が社内で問題となり、配属を希望していたチームとは別のチームへの配属となりました。ホスティングサービスの担当となり、Ansible を書いたりE2Eテストやインフラテストの自動化を行ったりとかなり充実してました。また、動いているものに対する責任を持つインフラエンジニアとして大切な姿勢を徹底的に叩き込まれて配属に感謝。また、組織の中で仕事をしていく上で技術力だけで通用する割合の低さとカオスな環境に放り込まれて「コード読めばどうにかなる」はFake野郎の言論であることは本当に勉強になりました。辞メンター「やめんたー」と呼びます。別に呼び方はどうでも良いのですが、新卒2年目突入して直ぐに尊敬していたメンターの方が退職されて、その方が持っていた運用と開発業務を一気に巻き取ることになりました。日頃のドキュメント管理の重要性を学びました。この頃から一人でさまざまなタスクを任されたり、オンコールに入ったりと中途半端な仕事が許されなくなってきました。そして、それらを誇りに思ってました。登壇とエバンジェリスト日々の業務の運用・開発は楽しく満足していたのですが、自分の成果を社内の人に話してもイマイチ反応が悪かったので外部の方と話をしたくなって外部登壇をするようになりました。その一環でサイバーセキュリティに関するソフトウェア開発や研究、実験、発表を一年間継続してモノづくりをする長期ハッカソンSecHack365に会社の支援をいただきながら参加したりもしました。本当にありがとうございました。ブランディングが目的でも自身の登壇やブログの執筆、研修講師、イベント準備を業務時間で行えて、それらが個人の評価に繋がるのは嬉しかったです。自分ともう一人のメンバーはITエンジニア本大賞2021の技術書部門とかに選ばれて格の違いというのを見せつけられましたが非常に刺激になりました。転職なぜ、やめるのか？主にこのパートでは誰かの評価が落ちることは覚悟しなければならないのですがあまり言及したくないので退職エントリーの中で一番面白く、皆様が楽しみにしているこのパートは飛ばします。現職のどこに不満を持っていて、そのうち、どこに改善の余地があり、妥協するべきであり、現職でそれが叶わず。次の職では何を期待するのか、についての思慮を重ねた結果としか言えません。そんなに大きな不満もなかったのですが普通に対面であればいくらでも喋るので気になる場合は飲みにでも誘って聞いてください。syu-m-5151.hatenablog.comまとめ雑な報告となりましたがこれからも、クラウドネイティブコミュニティ界隈ですし、登壇も続けていこうと思います。また、皆様とは違う形で出会えると思いますが「さよなら、インターネットと思春期」の終わりにです。","link":"https://syu-m-5151.hatenablog.com/entry/2021/05/31/094928","isoDate":"2021-05-31T00:49:28.000Z","dateMiliSeconds":1622422168000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#JTF2021w にて「Kubernetes Operator の実装と憂鬱」というタイトルで登壇しました。","contentSnippet":"推しテク総選挙 と聞いて 推しの Kubernetes Operator について語る為に登壇しました。資料を作ってたら概要というか想定から事前知識が多すぎて入門記事になってて途中で申し訳ないという感情になりました。この手の基礎がめちゃくちゃ多い場合にはある程度ペルソナ作って喋ることが大事そう… speakerdeck.com本セッションでは kubebuilder を用いた開発を紹介しました。公式ドキュメント は知見の塊のような存在なので何度読んでも良いと思います。また、Kubernetes はインフラ技術の総合格闘技だという言論があると思うが Kubernetes Operator の開発は パンクラチオン だと言えると思うほどに様々な状況があり、それらすべてを実装していくのは長い道のりである丘の半分に行くことにすら大きな労力が必要である。今回はそのような苦悩を発表したかったのですがちょっと至らなかったと思います。気が向いたら追記するが人間の気が向いたら追記するは二度と追記されない。","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/25/094725","isoDate":"2021-03-25T00:47:25.000Z","dateMiliSeconds":1616633245000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#技育祭 ではじめてでもわかる！コンテナ入門 という講義をしたのですがコンテナ技術以前に何が必要か？","contentSnippet":"概要空腹状態から回復する為に飯を食っても腹が出る人類の不思議について考えていたら夜でした。2021年03月13日 の15:50 から16:50 までの一時間で技育祭というイベントでコンテナの入門講義を行った。以前から事前に情報が共有できない入門講義は罪深いとは思っている。事前知識のある有無や特定のコンテキストへの理解など参加者のスタートラインを確認できないからで講義時間の関係上それらに一線を引いてここからあそこまでの入門講義だと思い込んで講義を行わなければならない。これは地味にシンドイ。講義する側になると予習復習は大事というのが身に染みます。講義の感想にたまにこんな声が聞こえる。コンテナ技術以前の前提知識が必要だと思うこういうことを聞くとオジサンなので彼らの為になるようにXXといういい本があるよ!!! インフラの勉強をするにはこういう名著がある。もちろん、基本的な知識としてネットワーク・ルーティング・スイッチング・ファイアウォール・負荷分散・高可用性・障害復旧・TCPやUDPのサービス・複数のUNIX・複数のウェブサーバー・キャッシュ・データベース・ストレージインフラ・暗号・アルゴリズム・キャパシティ計画立案に精通した人材になることが必要だよと、本格的に這い寄ると彼らは逃げる。なぜなら、彼らはインフラエンジニアになりたいわけではないのだから!!!では、何が必要なのか？ちょっと考えてみる。おまけ資料 speakerdeck.comとその投稿2021年03月13日 #技育祭 にて 15:50 -16:50 より はじめてでもわかる！コンテナ入門 というタイトルで登壇するのでそちらの資料になります。 https://t.co/40b29kdNHZ— nwiizo (@nwiizo) 2021年3月13日  神様ではないので分かりません。エリック・レイモンドはエッセイ「How To Become A Hacker」の中でどんな言語を勉強すべきかを述べている。まずPythonとJavaから始めよ、学ぶのが容易だから。真剣なハッカーはさらに、UnixをハックするためにCを学び、システム管理とCGIスクリプトのためにPerlを学ぶべし。と言っている。 実はこの主張は2007年の公開から少しずつ変更されている。大きな変更として学習すべき言語をJavaからGolang へ変更したことである。まぁ勉強すべき言語や方法はその時期などによって全く異なるということである。ハッカーになろう の基本的なハッキング技術 から何となく環境がちょくちょく変わっているその雰囲気が伝わると嬉しいです。まぁというようにエンジニアが学ぶべきことは年々変わる。素晴らしいハッカー(本来の意味での)たちが C言語を異常に勧めてくるのにはそれなりの理由があるのだが初学者には関係ない。彼らは強力なツールによって簡単に見かけ上の理解に到達できてしまい。それらに押しつぶされた経験がないからである。でも、最初からそんなに潰されたり一歩ずつ進む必要もない。小さな自信を積みながら楽しむやり方だってあるはずです!!!なので、勝手にコンテナ技術以前の前提知識 のオススメを紹介します。シェル芸芸は身を助くということわざがある。一芸を身につけておくと、いざというとき生計を助けることもあるという意味でつまりそういうことです。シェル芸 とは、主にUNIX系オペレーティングシステムにおいて「マウスも使わず、ソースコードも残さず、GUIツールを立ち上げる間もなく、あらゆる調査・計算・テキスト処理を CLI端末へのコマンド入力 一撃で 終わらせること」（USP友の会会長・上田隆一による定義）しかし、これだけは言いたいシェル芸 に助けられるばかりのコンテナ生活です。シェル芸を学ぶと各種コマンドの性質を掴めるようになります。将棋でいうと一手詰め将棋のようなものです()。基本的にコンテナも最終的にはシェルが動作しますのでこれらを学ぶことにあまり損は感じません。こちらの資料は最高なのでぜひ読んでください。    シェル芸初心者によるシェル芸入門  from icchy   www.slideshare.net基本的な構築と自動化...なんだかんだ言ってもインフラの構築経験がなければDockerfile に何を記載すればプロセスが動作するのか分からないと思います。なのでConoHa (宣伝含む) などに好きなミドルウェア(Nginx やApacheなど他多数) をインストールして設定をして様々な設定を遊んでみることも大事かと思います。あとは、自動で構築するのにもある程度慣れた方がいいのでAnsibleないしは類似したツールを使ってみることをオススメします。Dockerfile と違って都度実行結果が異なることも多いのでそこに考慮した書き方は将来的に役に立ってくると思います。Linux の基礎知識LPIC と LinuC に関しては勉強してあまり損しないと思います。レベル1ぐらい持っていても罰は当たりません。最後に好き勝手にいいましたがTwitterで有識者がもっと素晴らしい意見を言ってくれることを祈っています。今日はもう寝ます。おやすみなさい。","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/14/005638","isoDate":"2021-03-13T15:56:38.000Z","dateMiliSeconds":1615650998000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"#CNDO2021 Docker/Kubernetes 開発・運用のためにテスト駆動開発入門 を振り返っていたら朝。","contentSnippet":"もう、朝寝れてないのでついでに明後日の登壇資料の準備をしている。あとは謎の挙動にぶつかってLDAP にキレていたのを Sleep で切り抜けたので明日、聡明な自分が解決してくれることを信じてます。2021年3月11日にCLOUDNATIVE DAYS SPRING 2021 ONLINE に登壇しました(事前収録)。発表では 主に Docker/Kubernetes でのCI 周りのツールの紹介などを行いましたが日々のコンテナイメージのレビューに憑かれた人に向けて多少楽になる一助になればいいと思いました(ほんなこつ???)が時間管理が無限に下手で本当に入門なだけになってしまいました。結局、言いたいことの骨子をまとめてないからこんなことになるんやぞ!!!event.cloudnativedays.jp speakerdeck.comあとは所属組織でCloudNative Days Spring 2021 Onlineにトップスポンサーで協賛をしておりその関連で幕間CMを撮りました。これを見た幼稚園の頃からの幼馴染からインスタ経由でDMが来ました。ソフトウェアエンジニアとして正しい仕事しかしたくないみたいな感情は1年目になくなったのでなんでもやります。登壇資料よりこっちの方が伸びていて複雑な感情になった。やってました… 最後のジャンプだけ見てください…https://t.co/AKJ5UkKkcC https://t.co/TuwMv6cBTk— nwiizo (@nwiizo) 2021年3月11日  今回は登壇にあたって選考などはなく登壇者は動画を送れば終わりである。最初に聞いた時にはどうすんだろ...とギョッとしましたが1日目を終えた時には多様な価値観や発表を聞いてそれらが杞憂であると確信できました。また、公式ブログには以下のような文章があります。『今ならオンラインの特性を生かして、CloudNative Daysをダイナミックな環境でスケーラブルな形に更に進化させられるのではないか？』オンラインでは、誰でも情報を得ることができ、誰もが発信することもできます。オープンな思想のもとに作られたインターネットには境界がありません。そうしたインターネットの成り立ちを思い出し、初心者から達人まで、住んでいる場所を問わず、クラウドネイティブに取り組む人が、・今まで参加者だった人が壁を感じずに発信できる一日目を終えただけですが今回はどの発表も素敵で、非常に多様化していると感じました。しかし、自由であるが故に同じ時間と場所を共有できないコミュニケーションの難しさを感じました、。資料や発表に対する意見を聞けずに悲しい顔をしてます。資料で紹介したツールの紹介DockerDocker  は DockerfileをbuildしてImageを生成して Imageを実行してcontainer が爆誕するという大まかなながれがあるので確認ポイントがDockerfile、DockerImage、実際の環境での3つある。実際の環境での確認は環境にもよるがeBPF とか言い出さなければならないので今回はスコープから外します。hadolintベストプラクティスのDockerイメージを構築するのに役立つよりスマートなDockerfile Linter です。設定ファイルを置くことで特定のルールは無視することができるのでCIにも組み込みやすいと思います。ちなみに、hadolint から指摘されすぎて hadolint 先輩って呼んでます。dockleベストプラクティスのDockerイメージや セキュアなイメージを構築するのに役立つDocker Image Linter です。hadolint との違いは hadolint はDockerfileに対してのLinterなのに対してdockle はDocker Imageに対するLinter であることです。Event を拾う場所、適用する場所が違います。ある程度似たようなことも言ってくるのですが、はじめ易さでいうと hadolint に軍配があるとは思います(独自的偏見)。Trivyコンテナ脆弱性スキャンツールで、コンテナイメージからコンテナの OS パッケージやアプリケーションの依存ライブラリの脆弱性を検出してくれます。レポートの仕方も多様で滅茶苦茶にユーザーの事を考えているツールだと思います。techcrunch.comcontainer-structure-testコンテナ内部でコマンドを実行することで正しい出力やエラーが帰ってくるかどうかや、コンテナ内部のファイルが正しく格納されているかなどの検証を実行できるフレームワークです。Goss っぽいことができます。内部でコマンド実行して結果を確認するだけなんでコマンドで確認できるものは確認できます。ちょっとしたテストだとこれでどうにかなります。ShellCheck資料にないし登壇でも言及してないが非インフラエンジニア も entrypoint.sh などでシェルスクリプトを書く機会が増えると思う。要出典ではあるのだがシェルスクリプトは普通に動いてくれるので想定外の処理 を埋め込んでしまうことが多々ある。そんな時に頼りになるのがshellcheck である。不用意なrm などを諫めてくれたり変数の取り扱いなど良くハマるあれやこれやを指摘してくれる。頼もしいKubernetesKubernetes のマニュフェスト を確認できるツールは大きく分類すると主に3つのカテゴリに分類できます。API Validators とは Kubernetes APIサーバーに対して特定のYAMLマニフェストを検証Built-in checkers とは セキュリティ、ベストプラクティスなどの決まったものの検証を行うCustom validators  とは自らでルールや規約を作成して検証を行う、API Validators と違ってURLの重複チェックなどができないが大体気軽。kubeval Built-in checkerskubeval は、Kubernetes manifest のファイルを検証するために使用され、単純な記述ミスを検知することができます。kubectlには kubectl apply --validate=true --dry-run=true -f manifest.yaml で検証を行う事ができます。kubectlを使った検証方法では実際に対象のmanifestファイルを実行するため権限が必要kube-score Built-in checkerskube-score は、Kubernetes manifest のファイルを分析し、スコアを付けされますセキュリティの推奨事項とベストプラクティスに基づいてチェックされこれらを選択することができます。conftest Custom validatorsconftest は YAML や JSON などの構造化データに対してユニットテストを記述できるツールです。ポリシーはOpen Policy Agent (OPA) で使われているポリシー言語 Rego で記載することができます。柔軟性が高くyaml の確認ができるので様々なツールで利用可能で組織横断で使う設定を決めてしまえるのでかなり、オススメ!!!!!さいごにおわり、。特に振り返りもしてないけど もう眠い。宣伝グループ内でいくつか発表があったので紹介させてください。感想は気が向けばあとから追記します。｢PGマルチペイメントサービス」のマイクロサービス移行計画event.cloudnativedays.jp決済システムにおけるクラウドネイティブへの挑戦event.cloudnativedays.jpインフラ目線でみた、初めてコンテナでサービスをリリースする時のセキュリティポイントevent.cloudnativedays.jp最近、推してる音楽歌詞と歌声があまりにも最高なので聞いてくれ!!!!!!!!年老いて眠くなる死ぬ前になってわかる人は何を恐れて夢み今は生きてることに感心だつまらない感情抱えた今最低なことばかりじゃないyoutu.beOpen Policy Agent Rego Knowledge Sharing Meetup別でOPAのイベントがあったようなのでぜひ、みて欲しくて動画を追加しておきました。youtu.be","link":"https://syu-m-5151.hatenablog.com/entry/2021/03/12/044315","isoDate":"2021-03-11T19:43:15.000Z","dateMiliSeconds":1615491795000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"プログラミングの文法やインフラの知識は増えてやれることは増えたが、作っていくものが良いシステムにならない気がしている社会人エンジニア歴4年目の抱負を言いたい。","contentSnippet":"はじめにプログラミングの文法やウェブオペレーションを中心としたインフラの知識は増えて、障害発生しやすかったり性能が悪いコードや設定の書き方が徐々に分かってきてました。やりたいこと、やりたくないことが自分の中で徐々に理解できるようになってきました。しかし、そうやっていろんなものを作っていく中でそれらが本当に意味のある良いシステムにならない気がしている。ここから先へはどのようなステップアップで進めていけばいいか分からない。10年や20年後にこのブログを読んで振り返りたいと思うので社会人エンジニア歴4年目の抱負を言いたい。誰やねん2017年4月に新卒に技術職としてGMOインターネットに入社して4年目でソフトウェアエンジニアをやっている。インフラ開発と運用が主な業務です。インフラを良くしていくためのサーバーサイドを書いたり、言語はわりとなんでもやっています 。技術広報も拝命しており積極的に外部登壇をしたり社内で勉強会をしたりしている。もともと、入社当初は シェルスクリプト と Python を多用していましたが最近は個人用のツール系でどのような規模のものでもGo で書くようにしてます。Python を使う場面はとても減りました(シェルスクリプトは使いますがGo側からよびだすことが増えました)。個人の生産性の観点でも100行以上である場合にはシェルスクリプトや Python より Go のほうが上にななってきた感じです。IT界隈ではない方も住んでいるシェアハウスに住んでおり筋トレと格闘技が趣味で一昨年までいろんな場所でアマチュアの試合やったりしてました。酒は好きだが直ぐに酔っ払い粗相をするのであまり飲まない。2021年抱負仕事やっていく。一昨年の途中から特定のプロダクトや技術領域に関してリードエンジニアとして関わっているようになったので再度、自分の役割を見直してよりよいシステムを作っていく為に前提、原則、思想、習慣、視点、手法、法則 など普遍的定説的本質的に最適な行動が取れるように様々な分野や媒体での学習を続ける。毎日 GitHub に commit をする。技術の習得には input と output が大事で、input で得たものが output によって定着するのではないかという持論はあります。しかし、社内で作っているものはGithub管理ではないのでどうしたものかと思ってます。あとは、人間っぽいイベントはあきらめたくないなぁ～とも思ってます。79 ㎏ への ダイエット。行き過ぎた筋トレと減量反動による食べ過ぎにより人生で最も大きくなっています。このままでは健康を害する。雑でもいいので読んだ本のブログないしはまとめた内容を文章にする。去年が漫画を除くと 60冊ぐらい読んだのですが明確にOutPut を意識することはなかったのでこちらはやっていきたいと思ってます。お金や人生、将来について計画を立てる。結婚できないなら結婚できないなりに未来についてちゃんと考える。さいごにそんなに未来のことは何もわからないのでこの辺が今年の抱負かと思う。手を動かして勤勉にやっていくための健康を確保しながらinput と output の精度と量を上げて行きたいと思いました。","link":"https://syu-m-5151.hatenablog.com/entry/2021/01/04/162322","isoDate":"2021-01-04T07:23:22.000Z","dateMiliSeconds":1609745002000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"自分用 の2020 年の登壇などの振り返り","contentSnippet":"2020 年の振り返りを雑多に書いていきたいと思います。はじめに2020 年もコンテナ関連のインフラ技術を主軸として運用及び開発を行ってきました。その他にもReact やFlutter の開発ができたりしました。仕事、プライベート、コミュニティにお付き合いあった方々、ありがとうございました。2021 年もnwiizo をどうぞよろしくお願いいたします。2020 年の振り返り1 月12 月から1月までは社内で新規のコンテナ案件が増え、関わるステークホルダーが増えました。それらの方々を対象に同じ資料を使って3回程度社内勉強会をしておりました。 speakerdeck.comほかにもMail Hosting のスパムメールツール関連の 大きな改修とリリースがあったみたいですが記憶がありません。2 月セキュ鉄OWASP九州合同勉強会 にて勉強会を行いました。正直、過去一レベルで失敗しました。正直、よっぽどの理由がない限り環境構築はした状態でハンズオンを始めた方が良いという強い意志を固めました。secsteel.connpass.com speakerdeck.comプライベートではNginx Unit の検証をしており、仕事ではArgoCD Workflow の検証をひたすらにしてました。Workflow やKubernetes scheduling 周りについて興味を持っていて雑なコードが書き殴られていました。3 月外部登壇なしです。GMOインターネットが保持する商材サイトのKubernetes 化を行っていた記憶があります。concourse やDiDの検証や開発者の開発環境を作ったりしてました。4 月Infra Study Meetup #1「Infrastructure as Code」 〜インフラ技術の「これまで」と「これから」を網羅！インフラ勉強会シリーズ第1弾〜 で登壇して、インフラにおけるテストについて雑多にまとめた資料を作りました。ちょうど、この時期は既存環境の温かみのある手順書 + Nagios から Terraform + Ansible + Goss + Prometheus に構成を変えるみたいなことをしており、自分の中でも整理がついたので登壇して、めちゃくちゃよかったです。 speakerdeck.com上記で話したようにいにしえの環境と戦っていたのですが一部コードの修正が必要だったので書き換えとかもやってました。業務後にLSP とかのある環境で開発した時に生産性の向上を感じることができた。LSP最高である。5 月今年もGMO Technology Boot Camp という社内の新卒エンジニアの技術力向上・適性判断を目的とした研修プログラム にてコンテナ技術の講義の担当をした。 speakerdeck.comPrometheus Meetup Tokyo #4 ではPrometheus Operator について少しお話をしました。本当に良くない資料だと思う。 speakerdeck.com5月以降で登壇が完全にリモートになり工夫が足らず完全に登壇に対してネガティブな感情が募っていました。6,7,8 月本当に虚無である。強いていうなら kubebuilder をひたすらにいじっておりました。9 月2018年の修了でOpenFaaS のマルチユーザー対応してました。SecHack365 Beyond に登壇しました。 speakerdeck.com久しぶりに働き方などについて真剣に考えた。NO HARD WORK!　無駄ゼロで結果を出すぼくらの働き方 を読んだりその仕事、全部やめてみよう――１％の本質をつかむ「シンプルな考え方」 などを読んだり自分自身が4年間社会人をやった感想みたいなことを喋って時間が足りなくなった。自作 Kubernetes Operator による監視/通知自動化周りでリリースをこなした。10月Kubernetes Meetup Tokyo #35 にて Kubernetes API との邂逅 あるいは kubewebhook 入門 というタイトルで発表しました。kubewebhook は最高なので…。 speakerdeck.com仕事はクラスター周りのトラブルシューティングをひたすらにやっていた。11月Kubebuilder を用いて単純なCRUD API のOperator を実装していく上で必要な知識や様々な概念の説明 をしたかったのですが要約する場所が全然分からず自分の力不足を感じました。 speakerdeck.com12月はじめてでもわかる！コンテナ入門 - 社内研修研修公開しちゃいます - というイベントで講師しました。もう飽きたので終わり speakerdeck.com","link":"https://syu-m-5151.hatenablog.com/entry/2020/12/30/181640","isoDate":"2020-12-30T09:16:40.000Z","dateMiliSeconds":1609319800000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubernetes で readOnlyRootFilesystem の設定がTrue になってないのをデプロイする前にConftest したい","contentSnippet":"概要Kubernetes ではポッドセキュリティポリシーにより、ポッドの作成と更新のきめ細かい承認が可能になります。また、Pod に直接設定するだけではなくRBAC を利用したデフォルトの設定を行うことができたり、OPAのgatekeepeを利用することで開発者は意識することなくセキュリティの設定を行うことができます。こちらにはreadOnlyRootFilesystemというオプションがあり、このオプションを有効にするとコンテナ内がreadonlyになるためセキュリティの施策の１つとして有効にすべき場面が多いと思います。こちらは有用ですが yamlを作成していざ、本番へというタイミングでしか分からないのは流石にしんどいです。そのため、デプロイの前にこれはテストをすることは無駄ではありません。readOnlyRootFilesystem をやってみるapiVersion: v1kind: Podmetadata:  name: read-onlyroot-filesystemspec:  containers:  - name: centos7    image: centos:7    command: [ \"sh\", \"-c\", \"sleep 1h\" ]    securityContext:      readOnlyRootFilesystem: true  - name: centos8    image: centos:8    command: [ \"sh\", \"-c\", \"sleep 1h\" ]Podの作成kubectl apply -f readonly_pod.yaml pod/read-onlyroot-filesystem createdkubectl get pod NAME                       READY   STATUS    RESTARTS   AGEread-onlyroot-filesystem   2/2     Running   0          15mファイルのWrite 及び Read/Write を実施するkubectl exec -it read-onlyroot-filesystem -c centos7 touch testfilekubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.touch: cannot touch 'testfile': Read-only file systemcommand terminated with exit code 1$ kubectl exec -it read-onlyroot-filesystem -c centos7 ls   kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead.anaconda-post.log  bin  dev  etc  home  lib  lib64  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var$ kubectl exec -it read-onlyroot-filesystem -c centos7 pwd       kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl kubectl exec [POD] -- [COMMAND] instead./テストするgatekeepe や ポッドセキュリティポリシー をユーザーやNamespace 単位で実施することにより開発者は意識することなくセキュリティの設定を行うことができますがyamlを作成していざ、本番へというタイミングでしか分からないので流石にめんどくさいです。そのため、デプロイの前にこれはテストをすることは無駄ではありません。conftest は YAML や JSON などの構造化データに対してユニットテストを記述できるツールです。install と実行$ wget https://github.com/open-policy-agent/conftest/releases/download/v0.21.0/conftest_0.21.0_Linux_x86_64.tar.gz$ tar xzf conftest_0.21.0_Linux_x86_64.tar.gz$ sudo mv conftest /usr/local/bininput.spec.containers.securityContext.readOnlyRootFilesystem が正であればこれで通ります。package maindeny[msg] {  input.kind == \"Pod\"  input.spec.containers.securityContext.readOnlyRootFilesystem  msg := \"Containers must not write\"}Pass しました$ conftest test readonly_pod.yaml1 test, 1 passed, 0 warnings, 0 failures, 0 exceptions書き込みは許可したい場合はnot input.spec.containers.securityContext.readOnlyRootFilesystemのような設定をぶちこんであげれば良いです。package maindeny[msg] {  input.kind == \"Pod\"  not input.spec.containers.securityContext.readOnlyRootFilesystem  msg := \"Containers must write\"}そして、実行すると、想定通りに失敗します。事前に分かるというメリットは以外に多くありますのでぜひ、皆さんの環境でも試してみてはいかがでしょうか？conftest test readonly_pod.yamlFAIL - readonly_pod.yaml - Containers must write1 test, 0 passed, 0 warnings, 1 failure, 0 exceptions","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/25/185946","isoDate":"2020-11-25T09:59:46.000Z","dateMiliSeconds":1606298386000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubebuilder の前にCRDへ入門する #osc20fk Kubernetes Operator の直観 予稿 (2)","contentSnippet":"2020年11月28日(土) に Open Source Conference 2020 Online/Fukuoka でKubernetes Operator の直観 というタイトルでセッションを行うための予稿である。前回、大切な考え方や説明や概念をすっ飛ばしていろいろ進めてきました。今回も特段説明する予定がないのでこの辺の知識が必要な場合には 実践入門 Kubernetesカスタムコントローラーへの道 や Programming Kubernetes: Developing Cloud-Native Applications などを読むとよいです。はじめにリソースとは何らかのオブジェクトを概念です。皆さんがよく知る概念としてDeploymentやPodsがリソースです。リソースはKubernetes APIを持ち、実際に配備されているオブジェクトとしてのPodsがリソースとしてのPodsに格納されます。もう一つの大切な概念としてオブジェクトというものがあります。オブジェクトとは持続的なエンティティのことで、Kubernetesクラスターの状態を定義します。オブジェクトとはデプロイされた実際のPodsやServiceのことです。Custom Resources は、Kubernetesの特別なリソースです。Kubernetesを通常の方法で使用する場合、そのようなリソースを作成する必要はありません。したがって、これは多くのユーザーにとってそれほど重要ではありませんがKubebuilder への入門に関しては重要です。Custom Resources は、特別なリソースではありますが特殊なことはできません。CRD (CustomResourceDefinitions) はテーブル CR (CustomResource) はオブジェクトで「リンゴ」のような各レコードのような関係になっていきます。ちなみにKubebuilderではcontroller-genというツールを利用して、Goで記述したstructからCRDを生成する方式を採用しています。controller-toolsはコントローラーを構築するためのgoライブラリのセットで controller-runtime は、コントローラーをビルドするためのgoライブラリのセットです。やっていくKind でcluster を作成するCluster を作成するために$ kind create cluster --name crdCreating cluster \"crd\" ... ✓ Ensuring node image (kindest/node:v1.18.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Set kubectl context to \"kind-crd\"You can now use your cluster with:kubectl cluster-info --context kind-crdThanks for using kind! 😊## clusters の確認$ kind get clusterscrd# コンフィグの生成$ kind get kubeconfig --name crd  > kubeconfig.yamlCRD をデプロイするsample-controller/artifacts/examples at master · kubernetes/sample-controller · GitHub に出てきた最も単純なサンプルがこちらです。apiVersion: apiextensions.k8s.io/v1beta1kind: CustomResourceDefinitionmetadata:  name: foos.samplecontroller.k8s.iospec:  group: samplecontroller.k8s.io  version: v1alpha1  names:    kind: Foo    plural: foos  scope: Namespacedそれに対応するリソースがこちらです。apiVersion: samplecontroller.k8s.io/v1alpha1kind: Foometadata:  name: example-foospec:  deploymentName: example-foo  replicas: 1実際にデプロイしてみましょうkubectl apply -f foo_crd.yamlcustomresourcedefinition.apiextensions.k8s.io/foos.samplecontroller.k8s.io createdkubectl apply -f foo_example.yaml foo.samplecontroller.k8s.io/example-foo created確認してみる$ kubectl get fooNAME          AGEexample-foo   10s最後に最小でデプロイして確認できることが分かりましたがAPIもなければ特に設定も行っていないのでetcd に情報が保存されているだけの状態になります。その他にもさまざまな機能がある。Advanced topics で いくつか説明している。詳しいことは公式ドキュメントを読んで欲しいです。なぜなら、これはKubebuilder の為のCRD入門なので!!!!! Finalizer はカスタムオブジェクト削除前の処理を定義します。Finalizerを定義することで、カスタムオブジェクトをkubectl deleteした時、実際に削除される前に実行すべき処理を定義できます。Validationはカスタムオブジェクトの設定値が要件を満たしているか、妥当性を確認することバリデーションをかけてチェックを行えます。Printerはkubectl get する際に表示するパラメータを定義することが可能です。SubresourceはCRDのAPIエンドポイントのサブリソースとして機能します。この辺は流石に知っておくと良いと思います。kubebuilder create apiコマンドで生成されたapi/v1/<**>_types.goを見てみると、XxSpec, XxStatus, Xx, XxListというような構造体が定義されており、// +kubebuilder:から始まるマーカーコメントが付与されています。 make manifestsを実行するとcontroller-genなどのツールによりこれらの構造体とマーカーを頼りにCRDの生成をおこないます。make manifests で生成されたcontroller-genなどから生成された CRD を読んでもマジでなんのことか意味分からないので先にCRD の学習をすることを強く勧めます","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/18/150126","isoDate":"2020-11-18T06:01:26.000Z","dateMiliSeconds":1605679286000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"kubeval でマニフェストを確認できるので質問する前に一度、確認お願いします。","contentSnippet":"最初に kubeval というツールを使って、Kubernetes のマニフェストをチェックできます。kubeval は、Kubernetes manifest のファイルを検証するために使用され、単純な記述ミスを検知することができます。yaml の記述ミスは目視だと普通に見逃すことが多いと思います。なので、開発ワークフローの一部やCI、ローカルで使用することで様々なやり取りを減らして開発を円滑に進めることができます。この記事ではkubeval の基本的な使い方を説明することになりますがこれを読み終えた後に皆様がkubeval をインストールしてくれるのを切に願っております。使い方としては標準入力として与えるかファイルを引数で与える形になります。kubeval <file> [file...] [flags]一般的なNginx のDeploymentであるnginx-deployment.yamlを用意します。apiVersion: apps/v1 # for versions before 1.9.0 use apps/v1beta2kind: Deploymentmetadata:  name: nginx-deploymentspec:  selector:    matchLabels:      app: nginx  replicas: 2 # tells deployment to run 2 pods matching the template  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80kubeval インストールInstalling Kubeval よりインストールを行うことが出来ます(関係ないですがGoReleaser 滅茶苦茶便利ですよね～）。kubeval 実行kubeval はファイルをコマンドへ直接渡すか、標準入力として与えてあげることで実行できます。標準入力で渡してあげればよいということはkustomizeやhelm でファイルを生成する場合でも簡単にCIなどに載せたりチェックすることができるということです。嬉しいですよね!?!?!??$ cat nginx-deployment.yaml | kubevalPASS - stdin contains a valid Deployment (nginx-deployment)$ kubeval nginx-deployment.yaml PASS - nginx-deployment.yaml contains a valid Deployment (nginx-deployment)例えば、replicas: 2 の前に無駄な空白がある下記のような場合があります。spec:  selector:    matchLabels:      app: nginx   replicas: 2 # tells deployment to run 2 pods matching the template  template:実行してみると構文エラーが検出できていると思います。kubeval err-nginx-deployment.yamlERR  - Failed to decode YAML from err-nginx-deployment.yaml: error converting YAML to JSON: yaml: line 8: did not find expected key次のパターンでは 数字型を与えているはずの部分に下記のように数字を与えてしまっています。    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: \"80\"実行してみるとワーニングが発生しており型errorを検出できていると思います。kubeval miss-nginx-deployment.yamlWARN - miss-nginx-deployment.yaml contains an invalid Deployment (miss-nginx-deployment) - spec.template.spec.containers.0.ports.0.containerPort: Invalid type. Expected: integer, given: stringちなみにkubevalはKubernetesAPIから生成されたスキーマに依存しております。そのため、CRDのリソースは検証できません。--ignore-missing-schemas や --skip-kinds で検証を飛ばすことが推奨されております。vim で使うvim でプログラムを実行する場合にはノーマルモードで!をつけることで外部プログラムを実行できる 例としては:!ls でlsを実行できます。ちなみに、現在編集中ファイルの省略記号である%を使うことで、編集中へのプログラム実行も簡単にできるので下記のコマンドで簡単に編集中の環境もチェックすることができます。:!kubeval %最後にkubeval でマニフェストを確認できるので質問する前に一度、確認お願いします。お互いの為に。","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/08/213821","isoDate":"2020-11-08T12:38:21.000Z","dateMiliSeconds":1604839101000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"Kubebuilder と触れ合う #osc20fk Kubernetes Operator の直観 予稿 (1) ","contentSnippet":"2020年11月28日(土) に Open Source Conference 2020 Online/Fukuoka でKubernetes Operator の直観 というタイトルでセッションを行う。レベルは入門編として対象者はKubernetes Operator の開発運用を検討している人で前提知識についてはKubernetes に関する書籍を何となく理解できる方としているがそんなレベル感で聞いてくれる酔狂な人間はなかなかいないと思う。そのため、何回かに分けて Kubebuilder やその周辺知識について解説したいと思いますが図も何もない中でこんな文章読まされても意味ないので絶対に読んでほしいとかもありません。ちなみに、書籍であれば 実践入門 Kubernetesカスタムコントローラーへの道 や Programming Kubernetes: Developing Cloud-Native Applications などを読むとよいのでオススメです。はじめにKubernetesには、分散システムを構築、運用に必要な機能を提供する多くのAPIがあります。しかし、これらは意図的に汎用であり、およそ80％のユースケースを対象としています。Kubernetesに存在するアドオンと拡張機能の豊富なエコシステムを利用すると、重要な新機能を追加し、クラスタを拡張することができる。Kubernetes の動作をカスタマイズするには、Continuous Integration 側ではconftest や kubeval などを用いてポリシーを定義し、マニフェストが定義したポリシーを満たしているかテストする方法がある。これは導入コストは低いが強制力も弱い。Kubernetes が提供しているAPI機能を拡張する為の方法としてgatekeeper や Pod Security Policiesを用いることで同様にAPI Server 側のチェックを行う事ができる。導入コストはそここで強制力がある。しかし、柔軟性に欠ける。それ以外にもKubernetesが提供しているAPI機能を拡張する為の方法として主に次のモノが挙げられる。Admission WebhookCRDAPI Aggregation今回のお題はAdmission Webhook や CRD の開発を円滑に進める為のツールの Kubebuilder を用いて実際に作成した何もしないリソースをデプロイしたいと思います。Kubebuilder についてKubernetes 向けのAdmission Webhook は比較的にシュッと開発することができますがCustom Resourcesは、Kubernetesの設計コンセプトを正しく理解する必要があり、たくさんのマニフェストを記述する必要もあるため、非常に難しいです。Kubebuilder  では、controller-tools,controller-runtimeを用いて抽象化したライブラリと、マニフェストを自動生成するツール群を提供することで、Kubernetesの設計コンセプトを完璧に正しく理解する必要もたくさんのマニュフェスト群も書かずに簡単にカスタムコントローラを開発できます。Kubebuilder は、API を構築するための以下のような開発者のワークフローを容易にすることを試みている。1.新しいプロジェクトディレクトリの作成2.1つ以上のリソースAPIをCRDとして作成し、リソースにフィールドを追加。3.コントローラにリコンサイルループを実装し、追加リソースを見る4.クラスタに対して実行してテストする（CRDをセルフインストールし、コントローラを自動的に起動する5.新しいフィールドとビジネスロジックをテストするために、ブートストラップされた統合テストを更新。6.提供されたDockerfileからコンテナをビルドして公開Kubebuilder の背景や設計思想については [KubeBuilder Design Principles] を読んでいただければと思います。ちなみに本来であれば、この後にKubernetes のアーキテクチャやAPI-Server について及びあるべき姿と実際の状態を比較して実際の状態をあるべき状態へと近づける Reconciliation Loop と呼ばれる処理についてやCRD についても説明した方が良いのですが ゼロから始めるKubernetes Controller などの資料があるのでそちらを参照下さい。環境構築Quick Start - The Kubebuilder Book を参考にしていただければ基本的に環境構築は問題ないと思います。kubebuilder installos=$(go env GOOS)arch=$(go env GOARCH)# download kubebuilder and extract it to tmp# curl -L https://go.kubebuilder.io/dl/2.3.1/linux/amd64 | tar -xz -C /tmp/curl -L https://go.kubebuilder.io/dl/2.3.1/${os}/${arch} | tar -xz -C /tmp/# move to a long-term location and put it on your path# (you'll need to set the KUBEBUILDER_ASSETS env var if you put it somewhere else)sudo mv /tmp/kubebuilder_2.3.1_${os}_${arch} /usr/local/kubebuilderexport PATH=$PATH:/usr/local/kubebuilder/binはい、インストールできたと思います。kubebuilder --helpDevelopment kit for building Kubernetes extensions and tools.Provides libraries and tools to create new projects, APIs and controllers.Includes tools for packaging artifacts into an installer container.Typical project lifecycle:- initialize a project:  kubebuilder init --domain example.com --license apache2 --owner \"The Kubernetes authors\"- create one or more a new resource APIs and add your code to them:  kubebuilder create api --group <group> --version <version> --kind <Kind>Create resource will prompt the user for if it should scaffold the Resource and / or Controller. To onlyscaffold a Controller for an existing Resource, select \"n\" for Resource. To only definethe schema for a Resource without writing a Controller, select \"n\" for Controller.After the scaffold is written, api will run make on the project.Usage:  kubebuilder [command]Available Commands:  create      Scaffold a Kubernetes API or webhook.  edit        This command will edit the project configuration  help        Help about any command  init        Initialize a new project  version     Print the kubebuilder versionFlags:  -h, --help   help for kubebuilderUse \"kubebuilder [command] --help\" for more information about a command.kubebuilder initkubebuilder init で プロジェクトを作成したいと思います。* --domain : APIグループのドメインを指定* --license : ソフトウェアライセンスを選択* --owner : このソフトウェアのオーナーを指定$ go mod init kubebuilder-test-controller $ kubebuilder init --domain nwiizo.dev --license apache2 --owner nwiizoWriting scaffold for you to edit...Get controller runtime:$ go get sigs.k8s.io/controller-runtime@v0.5.0Update go.mod:$ go mod tidyRunning make:$ make/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./...go build -o bin/manager main.goNext: define a resource with:$ kubebuilder create api成果物の確認$ tree.├── Dockerfile├── Makefile├── PROJECT├── README.md├── bin│   └── manager├── config│   ├── certmanager│   │   ├── certificate.yaml│   │   ├── kustomization.yaml│   │   └── kustomizeconfig.yaml│   ├── default│   │   ├── kustomization.yaml│   │   ├── manager_auth_proxy_patch.yaml│   │   ├── manager_webhook_patch.yaml│   │   └── webhookcainjection_patch.yaml│   ├── manager│   │   ├── kustomization.yaml│   │   └── manager.yaml│   ├── prometheus│   │   ├── kustomization.yaml│   │   └── monitor.yaml│   ├── rbac│   │   ├── auth_proxy_client_clusterrole.yaml│   │   ├── auth_proxy_role.yaml│   │   ├── auth_proxy_role_binding.yaml│   │   ├── auth_proxy_service.yaml│   │   ├── kustomization.yaml│   │   ├── leader_election_role.yaml│   │   ├── leader_election_role_binding.yaml│   │   └── role_binding.yaml│   └── webhook│       ├── kustomization.yaml│       ├── kustomizeconfig.yaml│       └── service.yaml├── go.mod├── go.sum├── hack│   └── boilerplate.go.txt└── main.gokubebuilder create apiAPI作成してリソースの定義するために  kubebuilder create api  を実行してきます。みんな大好き --help を使っていけば様々な情報が得られるので割愛しますが素晴らしい情報がたくさんあるのでよろしくお願いします。#$ kubebuilder create api --group ship --version v1beta1 --kind Frigate Create Resource [y/n]yCreate Controller [y/n]yWriting scaffold for you to edit...api/v1beta1/frigate_types.gocontrollers/frigate_controller.goRunning make:$ make/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./...go build -o bin/manager main.go実行するための環境構築 Kind でのcluster の構築手元のconfigから接続出来るKubernetes Cluster に入れていくためのCluster 作成しました。$ kind create cluster --name kubebuilder-test-controller # Cluster の作成 出力省略$ kind get clusters # Cluster の確認kubebuilder-test-controller$ kind get kubeconfig --name kubebuilder-test-controller  > kubeconfig.yamlmake installcontroller-gen を実行してCRD を実行します。$ make install~/go/bin/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\"./...\" output:crd:artifacts:config=config/crd/baseskustomize build config/crd | kubectl apply -f -customresourcedefinition.apiextensions.k8s.io/frigates.ship.nwiizo.dev createdmake runmake run を実行したらcontroller のビルドと実行が出来ており、先程作成したCRDのcontrollerとして機能していると思います。$ make run/home/smotouchi/go/bin/controller-gen object:headerFile=\"hack/boilerplate.go.txt\" paths=\"./...\"go fmt ./...go vet ./.../home/smotouchi/go/bin/controller-gen \"crd:trivialVersions=true\" rbac:roleName=manager-role webhook paths=\"./...\" output:crd:artifacts:config=config/crd/basesgo run ./main.go2020-11-02T06:45:33.772Z        INFO    controller-runtime.metrics      metrics server is starting to listen    {\"addr\": \":8080\"}2020-11-02T06:45:33.773Z        INFO    setup   starting manager2020-11-02T06:45:33.773Z        INFO    controller-runtime.manager      starting metrics server {\"path\": \"/metrics\"}2020-11-02T06:45:33.777Z        INFO    controller-runtime.controller   Starting EventSource    {\"controller\": \"frigate\", \"source\": \"kind source: /, Kind=\"}2020-11-02T06:45:33.877Z        INFO    controller-runtime.controller   Starting Controller     {\"controller\": \"frigate\"}2020-11-02T06:45:33.877Z        INFO    controller-runtime.controller   Starting workers        {\"controller\": \"frigate\", \"worker count\": 1}kubectl apply -f config/samples/ship_v1beta1_frigate.yamlkubectl apply -f config/samples/ship_v1beta1_frigate.yaml を実行したらcontroller のようなログが出てきていると思います。2020-11-02T06:49:54.186Z        DEBUG   controller-runtime.controller   Successfully Reconciled {\"controller\": \"frigate\", \"request\": \"default/frigate-sample\"}そして、frigate-sample  がデプロイされていることを確認できると思います。今回は何もしないリソースをデプロイしたので何もしません。何もしていない controllerを見ていきましょう$ kubectl get crd/frigates.ship.nwiizo.devNAME                       CREATED ATfrigates.ship.nwiizo.dev   2020-11-02T06:42:36Z$ kubectl get frigate NAME             AGEfrigate-sample   6m23s$ kubectl get frigate/frigate-sample -o yaml apiVersion: ship.nwiizo.dev/v1beta1kind: Frigatemetadata:  annotations:    kubectl.kubernetes.io/last-applied-configuration: |      {\"apiVersion\":\"ship.nwiizo.dev/v1beta1\",\"kind\":\"Frigate\",\"metadata\":{\"annotations\":{},\"name\":\"frigate-sample\",\"namespace\":\"default\"},\"spec\":{\"foo\":\"bar\"}}  creationTimestamp: \"2020-11-02T06:58:32Z\"  generation: 1  managedFields:  - apiVersion: ship.nwiizo.dev/v1beta1    fieldsType: FieldsV1    fieldsV1:      f:metadata:        f:annotations:          .: {}          f:kubectl.kubernetes.io/last-applied-configuration: {}      f:spec:        .: {}        f:foo: {}    manager: kubectl    operation: Update    time: \"2020-11-02T06:58:32Z\"  name: frigate-sample  namespace: default  resourceVersion: \"3165\"  selfLink: /apis/ship.nwiizo.dev/v1beta1/namespaces/default/frigates/frigate-sample  uid: 2daa801b-d728-4de8-b7f1-ae56ad5eab61spec:  foo: barこれで、なにもしないリソースをデプロイできたと思います。controllers/frigate_controller.go最後に controllers/frigate_controller.go の Reconcile の中は現在このようになっていると思います。各種 ロジックなどを実装したい場合にはReconcile の中に諸々を書いてき再度ビルドしなおせば基本的にはOkです。 37 // +kubebuilder:rbac:groups=ship.nwiizo.dev,resources=frigates,verbs=get;list;watch;create;update;patch;delete 38 // +kubebuilder:rbac:groups=ship.nwiizo.dev,resources=frigates/status,verbs=get;update;patch 39 40 func (r *FrigateReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) { 41 ▸---_ = context.Background() 42 ▸---_ = r.Log.WithValues(\"frigate\", req.NamespacedName) 43 44 ▸---// your logic here 45 46 ▸---return ctrl.Result{}, nil 47 }最後に次回のブログではReconcile の実装に触れてみたいと思います。","link":"https://syu-m-5151.hatenablog.com/entry/2020/11/02/170525","isoDate":"2020-11-02T08:05:25.000Z","dateMiliSeconds":1604304325000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"出勤前に kubewebhook を入門していくかと思ったら時間がないので手順だけ シュッと書く","contentSnippet":"「新しいビジネス様式 byGMO」と謳って在宅勤務を継続しながら出社勤務を再開したのですがあまりにも出社から遠のいた結果、何時に家を出れば何時に会社に到着するのか分からなくなった結果、めちゃくちゃ早く会社についたのでブログを書こうかと思うはじめにKubewebhook とは、Kubernetesのexternal admission webhooks を作成するための小さなGoフレームワークです。Kubewebhookを使用すると、Webhookの検証と変更を非常に高速に行うことができ、主にWebhook自体のドメインロジックに焦点を当てることができます。特徴としては下記がありますReady for mutating and validating webhook kinds (compatible with CRDs).Easy and testable API.Simple, extensible and flexible.Multiple webhooks on the same server.Webhook metrics (RED) for Prometheus with Grafana dashboard included.Webhook tracing with Opentracing.Type specific (static) webhooks and multitype (dynamic) webhooks.github のissues にも記載ありますが数ヶ月間本番環境で使用されているらしく 弊社() でも本番利用しております。本番運用で以下に運用していくかはk8s-webhook-example などをみていくつかのシステムに適合していくつかのエラー処理さえ追加すればよいのではないかなーって思います。環境構築cluster の構築 kind create cluster --name kubewebhook                                                                                                                                                   2250msCreating cluster \"kubewebhook\" ... ✓ Ensuring node image (kindest/node:v1.18.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Set kubectl context to \"kind-kubewebhook\"You can now use your cluster with:kubectl cluster-info --context kind-kubewebhookHave a nice day!cluster の確認$ kind get clusterskubewebhook# コンフィグの生成$ kind get kubeconfig --name kubewebhookapiVersion: v1clusters:- cluster:    certificate-authority-data: LS*****************************LS0tLQo=    server: https://127.0.0.1:35765  name: kind-kubewebhookcontexts:- context:    cluster: kind-kubewebhook    user: kind-kubewebhook  name: kind-kubewebhookcurrent-context: kind-kubewebhookkind: Configpreferences: {}users:- name: kind-kubewebhook  user:    client-certificate-data: LS0tLS1*************************************************************************S0tLQo=    client-key-data: LS0tLS1CRUd******************************************0tCg==$  kind get kubeconfig --name kubewebhook  > kubeconfig.yaml$ kubectl version  --kubeconfig=kubeconfig.yamlClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}Server Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.2\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"clean\", BuildDate:\"2020-04-30T20:19:45Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}証明書発行自己証明書、つまりオレオレ証明書を作って利用します。subj を設定しないとちゃんと設定しないと上手く動作しませんのでご注意ください。$ openssl genrsa -out webhookCA.key 2048$ openssl req -new -key ./webhookCA.key -subj \"/CN=pod-annotate-webhook.default.svc\" -out ./webhookCA.csr$ openssl x509 -req -days 365 -in webhookCA.csr -signkey webhookCA.key -out webhook.crt$ kubectl create secret generic \\    pod-annotate-webhook-certs \\    --from-file=key.pem=./webhookCA.key \\    --from-file=cert.pem=./webhook.crt \\    --dry-run -o yaml > ./deploy/webhook-certs.yaml証明書をデプロイ先程、生成したシークレットをデプロイします。$ kubectl apply -f ./deploy/webhook-certs.yamlWebhookをデプロイしますDocker のビルドとKind での読み込みをやってからWebhook を受け取る処理と設定の変更を行ってくれるサーバーをデプロイします。main.go の中で下記のような感じでPod に追加したいラベルや情報を付与していきます。    if pod.Labels == nil {        pod.Labels = make(map[string]string)    }    pod.Labels[\"webhook\"] = \"true\"$ docker build . -t kubewebhook$ kind load docker-image kubewebhook:latest --name kubewebhook$ kubectl apply -f ./deploy/webhook.yamlWebhookを登録します詳しくはDynamic Admission Control などをご覧ください。$ kubectl apply -f ./deploy/webhook-registration.yamlテストについてこのような よくあるyaml をデプロイしますapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-testspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginxデプロイしたら   \"mutated\": \"true\"および\"mutator\": \"pod-annotate\" がデプロイされているのが分かります。$ kubectl apply -f ./deploy/test-deployment.yaml$ kubectl get pod --show-labelsNAME                                    READY   STATUS    RESTARTS   AGE     LABELSnginx-test-f89759699-2f4pv              1/1     Running   0          3m51s   app=nginx,pod-template-hash=f89759699,webhook=truenginx-test-f89759699-hkxs9              1/1     Running   0          3m51s   app=nginx,pod-template-hash=f89759699,webhook=truenginx-test-f89759699-mj9bg              1/1     Running   0          3m51s   app=nginx,pod-template-hash=f89759699,webhook=truepod-annotate-webhook-64fcdc7758-lgbtd   1/1     Running   0          5m5s    app=pod-annotate-webhook,pod-template-hash=64fcdc7758$ kubectl get pods/nginx-test-f89759699-mj9bg -o json |  jq '.metadata.annotations' {  \"mutated\": \"true\",  \"mutator\": \"pod-annotate\"}検証自体はかなり前にやっていたのですがそのログをかき集めた形になるので多少雑になったのですが時間があれば普通に解説のブログ書きます。終わり。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2020/10/01/141409","isoDate":"2020-10-01T05:14:09.000Z","dateMiliSeconds":1601529249000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"夜は短い だが乙女がいないので Nginx Unit で静的ファイルを取得するまでのログ","contentSnippet":"これはNginx のお話ではなく普通のCentOS にNGINX Unit というミドルウェアをインストールして静的ファイルをいくつかのhosts で設定していくのを寝る前にやっていく今日は書き終えて寝たい。何としても床を得たい。環境構築現環境# cat /etc/redhat-releaseCentOS Linux release 8.2.2004 (Core)インストールnginx Unit のCentOS用の手順 を参考にしながら設定を行う/etc/yum.repos.d/unit.repo の設定/etc/yum.repos.d/unit.repo に下記の設定を入れる[unit]name=unit repobaseurl=https://packages.nginx.org/unit/centos/$releasever/$basearch/gpgcheck=0enabled=1パッケージインストールUnit パッケージと使用するその他のパッケージをインストールします# yum install unit -y# yum install unit-devel unit-go unit-jsc8 unit-jsc11 \\      unit-perl unit-php unit-python27 unit-python36 -y確認 及び起動と有効化インストールされているか各種確認を行い、設定の諸々の有効化を行います。あと、起動コマンドがあるのであれば -h もしくはそれらに準ずるオプションは絶対に入力をしましょう。人は救われることが大いにあります。# which unitd/usr/sbin/unitd# unitd --versionunit version: 1.19.0configured as ./configure --prefix=/usr --state=/var/lib/unit --control=unix:/var/run/unit/control.sock --pid=/var/run/unit/unit.pid --log=/var/log/unit/unit.log --tmp=/var/tmp --tests --openssl --modules=/usr/lib64/unit/modules --libdir=/usr/lib64 --cc-opt='-O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -Wp,-D_GLIBCXX_ASSERTIONS -fexceptions -fstack-protector-strong -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -specs=/usr/lib/rpm/redhat/redhat-annobin-cc1 -m64 -mtune=generic -fasynchronous-unwind-tables -fstack-clash-protection -fcf-protection -fPIC' --ld-opt='-Wl,-z,relro -Wl,-z,now -pie'人を救う-h での確認#  unitd -hunit options:  --version            print unit version and configure options  --no-daemon          run unit in non-daemon mode  --control ADDRESS    set address of control API socket                       default: \"unix:/var/run/unit/control.sock\"  --pid FILE           set pid filename                       default: \"/var/run/unit/unit.pid\"  --log FILE           set log filename                       default: \"/var/log/unit/unit.log\"  --modules DIRECTORY  set modules directory name                       default: \"/usr/lib64/unit/modules\"  --state DIRECTORY    set state directory name                       default: \"/var/lib/unit\"  --tmp DIRECTORY      set tmp directory name                       default: \"/var/tmp\"  --user USER          set non-privileged processes to run as specified user                       default: \"nobody\"  --group GROUP        set non-privileged processes to run as specified group                       default: user's primary group有効化及びスタートしていきます# systemctl enable unitCreated symlink /etc/systemd/system/multi-user.target.wants/unit.service → /usr/lib/systemd/system/unit.service.# systemctl start unit# ps auxf | grep [u]nitroot       27237  0.0  0.3  65968  5620 ?        Ss   23:02   0:00 unit: main v1.19.0 [/usr/sbin/unitd --log /var/log/unit/unit.log --pid /var/run/unit/unit.pid --no-daemon]nobody     27239  0.0  0.0  65968   664 ?        S    23:02   0:00  \\_ unit: controllernobody     27240  0.0  0.1  66108  2636 ?        S    23:02   0:00  \\_ unit: routerll /var/run/unit/control.socksrw------- 1 root root 0  9月 16 23:02 /var/run/unit/control.sockログの確認cat /var/log/unit/unit.log2020/09/16 23:02:40 [info] 27238#27238 discovery started2020/09/16 23:02:40 [notice] 27238#27238 module: java 11.0.6 \"/usr/lib64/unit/modules/java11.unit.so\"2020/09/16 23:02:40 [notice] 27238#27238 module: java 1.8.0_242 \"/usr/lib64/unit/modules/java8.unit.so\"2020/09/16 23:02:40 [notice] 27238#27238 module: perl 5.26.3 \"/usr/lib64/unit/modules/perl.unit.so\"2020/09/16 23:02:40 [notice] 27238#27238 module: php 7.2.11 \"/usr/lib64/unit/modules/php.unit.so\"2020/09/16 23:02:40 [notice] 27238#27238 module: python 2.7.16 \"/usr/lib64/unit/modules/python2.7.unit.so\"2020/09/16 23:02:40 [notice] 27238#27238 module: python 3.6.8 \"/usr/lib64/unit/modules/python3.6.unit.so\"2020/09/16 23:02:40 [info] 27237#27237 controller started2020/09/16 23:02:40 [notice] 27237#27237 process 27238 exited with code 02020/09/16 23:02:40 [info] 27240#27240 router started2020/09/16 23:02:40 [info] 27240#27240 OpenSSL 1.1.1c FIPS  28 May 2019, 1010103fQuick StartQuick Start を見ながら最初のアプリケーションをデプロイしていく# cat << EOF > config.json    {        \"type\": \"php\",        \"root\": \"/www/blogs/scripts\"    }EOF# curl -X PUT --data-binary @config.json --unix-socket /var/run/unit/control.sock http://localhost/config/applications/blogs{        \"error\": \"Failed to apply new configuration.\"}かなCのでログを見る。2020/09/16 23:22:03 [alert] 27309#27309 root realpath(/www/blogs/scripts) failed (2: No such file or directory)2020/09/16 23:22:03 [notice] 27237#27237 process 27309 exited with code 12020/09/16 23:22:03 [warn] 27240#27240 failed to start application \"blogs\"2020/09/16 23:22:03 [alert] 27240#27240 failed to apply new confあ、ディレクトリがありませんっておしゃってますね。作って再実行する。# mkdir -p /www/blogs/scripts# curl -X PUT --data-binary @config.json --unix-socket /var/run/unit/control.sock http://localhost/config/applications/blogs{        \"success\": \"Reconfiguration done.\"}次は成功した。ドキュメントへの過度な信頼はいつなくなったんだろう...。眠いので感傷的になっているかもしれないですが下記のコマンドで確認できます。# curl -XGET --unix-socket /var/run/unit/control.sock http://localhost/config/{        \"listeners\": {},        \"applications\": {                \"blogs\": {                        \"type\": \"php\",                        \"root\": \"/www/blogs/scripts\"                }        }}# curl -XDELETE --unix-socket /var/run/unit/control.sock http://localhost/config/{        \"success\": \"Reconfiguration done.\"}# curl -XGET --unix-socket /var/run/unit/control.sock http://localhost/config/{}各種メゾットがちゃんと役割を果たしていて個人的には非常に嬉しいです。ほかに言うことはありません。適当にアプリなど配置すれば確認できるのでしょうが今回はターゲットではないので設定の投入だけ確認できたので終わり静的なファイルの配置mkdir -p /www/data/static/ とかでファイルを作って index.html を直下に配置する。<!doctype html><html>  <head>    <title>This is the title of the webpage!</title>  </head>  <body>    <p>welcome to Nginx Unit </p>  </body></html># cat << EOF > config.json{    \"listeners\": {        \"127.0.0.1:8300\": {            \"pass\": \"routes\"        }     },    \"routes\": [        {            \"action\": {                \"share\": \"/www/data/static/\"             }        }    ]}EOF設定ファイルの設定を行い確認する。# curl -X PUT --data-binary @config.json --unix-socket /var/run/unit/control.sock http://localhost/config/{        \"success\": \"Reconfiguration done.\"}]# curl -XGET --unix-socket /var/run/unit/control.sock http://localhost/config/{        \"listeners\": {                \"127.0.0.1:8300\": {                        \"pass\": \"routes\"                }        },        \"routes\": [                {                        \"action\": {                                \"share\": \"/www/data/static/\"                        }                }        ]}curl  を行うポートが解放されているか確認する# netstat -nltp | grep [u]nittcp        0      0 127.0.0.1:8300          0.0.0.0:*               LISTEN      27240/unit: router実際に公開したものを取得してみる...# curl 127.0.0.1:8300/index.html<!doctype html><html>  <head>    <title>This is the title of the webpage!</title>  </head>  <body>    <p>welcome to Nginx Unit </p>  </body></html>一旦、検証のブログとしては完成しました(完成したんです)。本当は図などがあればいいのですがよいです。","link":"https://syu-m-5151.hatenablog.com/entry/2020/09/16/235359","isoDate":"2020-09-16T14:53:59.000Z","dateMiliSeconds":1600268039000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"k8s.io/metrics/pkg/client/clientset/versioned を使ってPod で利用している Memory を確認する","contentSnippet":"kubectl top pod と docker stats メモリー使用量の違いは、メモリー使用量にPage Cache(active)が含まれているためですがメモリを基準にさまざまな制御を行う事ってありますよね。これはライブアップ実施後の確認の間に書いてランチを食べれていない。はじめにkubernetes API にアクセスする時には様々な方法がある。kubectl や curl などでも行う事ができる。でも、kubectl 使うことが一般的だとは思う。しかし、もう少し複雑な事をさせたいと思ったらのなら何かしらのプログラミング言語で実施したくなります。Kubernetes APIにアクセスするにはk8s.io/client-goを使うのはわりと自然な流れかと思います。今回、やりたいこととしてはkubernetes APIから取得したデーターを基にメモリの使用量を取得したいと思ってます。さっそく、本題今回は本当にお腹空いてる。 kubectl top pod -n <ns> ちなみに詳細を知りたい場合にはkubectl  top pod -n <ns> -v9などを記載しているといい感じになる。今回は  GitHub - kubernetes/metrics: Kubernetes metrics-related API types and clients を利用してメモリの使用量を確認したいと思います。最終的なコードはこちらになります。package mainimport (        \"fmt\"        \"os\"        \"path/filepath\"        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"        \"k8s.io/client-go/tools/clientcmd\"        \"k8s.io/client-go/util/homedir\"        metrics \"k8s.io/metrics/pkg/client/clientset/versioned\")func main() {        var kubeconfig string //empty, assuming inClusterConfig        kubeconfig, ok := os.LookupEnv(\"KUBECONFIG\")        if !ok {                kubeconfig = filepath.Join(homedir.HomeDir(), \".kube\", \"config\")        }        config, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfig)        if err != nil {                panic(err)        }        mc, err := metrics.NewForConfig(config)        if err != nil {                panic(err)        }        podMetrics, err := mc.MetricsV1beta1().PodMetricses(metav1.NamespaceAll).List(metav1.ListOptions{})        if err != nil {                fmt.Println(\"Error:\", err)                return        }        for _, podMetric := range podMetrics.Items {                podContainers := podMetric.Containers                for _, container := range podContainers {                        memQuantity, ok := container.Usage.Memory().AsInt64()                        if !ok {                                return                        }                        msg := fmt.Sprintf(\"Container Name: %s \\n Memory usage: %d\", container.Name, memQuantity)                        fmt.Println(msg)                }        }}遊ぶ環境情報環境構築はとても大切です。kubeadm や minikube など他にも様々な手段がある。が 今回は Kindを利用します。$ go versiongo version go1.14.5 linux/amd64$ docker version                                                                                                                                                                                                                  4290msClient: Version:           19.03.8 API version:       1.40 Go version:        go1.13.8 Git commit:        afacb8b7f0 Built:             Tue Jun 23 22:26:12 2020 OS/Arch:           linux/amd64 Experimental:      falseServer: Engine:  Version:          19.03.11  API version:      1.40 (minimum version 1.12)  Go version:       go1.13.12  Git commit:       77e06fd  Built:            Mon Jun  8 20:24:59 2020  OS/Arch:          linux/amd64  Experimental:     false containerd:  Version:          v1.2.13  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429 runc:  Version:          1.0.0-rc10  GitCommit: docker-init:  Version:          0.18.0  GitCommit:        fec3683$ kind --version # https://kind.sigs.k8s.io/docs/user/quick-start/kind version 0.8.1環境構築クラスタ-の構築$ kind create cluster --name metricsCreating cluster \"metrics\" ... ✓ Ensuring node image (kindest/node:v1.18.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Set kubectl context to \"kind-metrics\"You can now use your cluster with:kubectl cluster-info --context kind-metricsNot sure what to do next? 😅  Check out https://kind.sigs.k8s.io/docs/user/quick-start/# kubeconfig の書き込み$ kind get kubeconfig --name metrics  > kubeconfig.yaml# kubeconfing の確認$ kubectl version  --kubeconfig=kubeconfig.yaml Client Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}Server Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.2\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"clean\", BuildDate:\"2020-04-30T20:19:45Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}metrics-server の導入観測用のPodのデプロイ$ kubectl create deployment nginx --image=nginxdeployment.apps/nginx created$ kubectl get podNAME                    READY   STATUS    RESTARTS   AGEnginx-f89759699-k2vc7   1/1     Running   0          39skubernetes v1.11からはmetrics serverを動かしてやればいいのですがいまいち上手く取得できないので こちらのIssue を参考にしたのがこちらです。$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yamlclusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader createdclusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator configuredrolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader configuredapiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io unchangedserviceaccount/metrics-server unchangeddeployment.apps/metrics-server configuredservice/metrics-server configuredclusterrole.rbac.authorization.k8s.io/system:metrics-server configuredclusterrolebinding.rbac.authorization.k8s.io/system:metrics-server unchanged実行$  kubectl top pod -A NAMESPACE            NAME                                            CPU(cores)   MEMORY(bytes)default              nginx-f89759699-k2vc7                           0m           3Mikube-system          coredns-66bff467f8-k85lw                        3m           8Mikube-system          coredns-66bff467f8-xlnz5                        3m           8Mikube-system          etcd-metrics-control-plane                      24m          55Mikube-system          kindnet-55z2v                                   2m           9Mikube-system          kube-apiserver-metrics-control-plane            41m          260Mikube-system          kube-controller-manager-metrics-control-plane   15m          39Mikube-system          kube-proxy-whtdr                                1m           12Mikube-system          kube-scheduler-metrics-control-plane            4m           15Mikube-system          metrics-server-697bddf786-55sq9                 1m           13Milocal-path-storage   local-path-provisioner-bd4bb6b75-8qfzn          11m          9Mi$ go run main.goContainer Name: coredns Memory usage: 9060352Container Name: kube-proxy Memory usage: 13139968Container Name: etcd Memory usage: 57958400Container Name: kube-controller-manager Memory usage: 40906752Container Name: local-path-provisioner Memory usage: 9605120Container Name: kube-apiserver Memory usage: 273580032Container Name: kube-scheduler Memory usage: 16199680Container Name: coredns Memory usage: 8982528Container Name: kindnet-cni Memory usage: 9945088Container Name: nginx Memory usage: 3526656Container Name: metrics-server Memory usage: 14118912コード解説は後で書くのと取得値がkubectl top pod と違う件はラーメン屋から帰ってきてから書きます。とりあえず、公開しておきます。最終成果物workspace_2020/blog/metrics at master · nwiizo/workspace_2020 · GitHub","link":"https://syu-m-5151.hatenablog.com/entry/2020/08/04/172142","isoDate":"2020-08-04T08:21:42.000Z","dateMiliSeconds":1596529302000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"GMO Developers Day 2020 で U30エンジニアが語る グループ各社の開発文化とマネジメント ではかく語りき","contentSnippet":"はじめに2020年7月28日の火曜日にGMO Developers Day 2020 で U30エンジニアが語る グループ各社の開発文化とマネジメント では パネルディスカッションのモデレーターを依頼されました。これまでパネリストとしての登壇経験はありましたがモデレーターとしては初登壇となります。gmo.connpass.com本セッションはグループ各社の若手エンジニアの労働やマネジメント、キャリアに対するアンソロジー的な内容になってる。同じ働き方というテーマでも 開発トップが考えるwithコロナでのチーム開発というのが控えている。開発組織、上から見るか横から見るかという趣がある。趣がありすぎます。あー、スライド作った。 pic.twitter.com/EPJGlrGmmL— nwiizo (@nwiizo) 2020年7月21日  いい笑顔要旨20世紀の階層的な官僚制は、生産性の飛躍的な向上を達成するために、たくさんの人間や組織が一緒に働くことをを可能にしました。その後、世界は大きく変わり、ソフトウェアは生産性の大幅な向上を可能にしましたが、かつてないほどの計り知れない複雑性をもたらしました。それらをどのように解決していくか、もしくはどのように向き合っているかが組織にとっては重要なものになってきていますが、言及していくと無限な広がりを見せていくので少し控えるとします。GMOインターネットグループは、対外的にみても中にいて働いていても梁山泊経営 を行っていると思います。そのため、大きなマインドという部分でいうとスピリットベンチャー宣言というものがありますが上記であげたような複雑性との向き合い方は各社ごとに変わってきます。テーマとして 開発文化、チームメンバーから見るマネジャー・シニアエンジニアに求めること、どういうマインドで若手はやっているのか (何を目指しているのか、モチベーションの持ち方、成長の仕方）を話し各組織がどのような形で向き合っていっているか相互理解や議論ができればいいなってまじめなことを思っていました。が、インフラエンジニアがモデレーターをやって、開発組織や文化が違うセキュリティエンジニア、フロントエンドエンジニア、バックエンドエンジニアで意見を交換するのは正直、コンテキストが合わない部分があるのではないかと思いますがそれらを含めて楽しんでいただければと思います。よく思い出すと、技術以外の分野やテーマについて語るのも、公の場でははじめてで緊張していますが登壇者がひたすらに優秀なので、なんとか形になるようにやっていこうと思います。","link":"https://syu-m-5151.hatenablog.com/entry/2020/07/27/140948","isoDate":"2020-07-27T05:09:48.000Z","dateMiliSeconds":1595826588000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"俺のvimrc 2020","contentSnippet":"概要PCを開くと、同時にターミナルが開く。そのタイミングでtmux のセッションがなけれな起動してアタッチする用に設定してある。僕のPCでは、ファイルを編集したい時には、必ずと言っていいほど、vimを使う。vim は生涯の友である。また、最近はtmuxで作業を分割することも増えたため、vim だけに収まらない環境構築男になった。2017年の1月 のあの頃よりやりたいことも増えた。まだまだ、素人の私は、玄人感の演出に余念がない。私は.vimrcを編集して更なる玄人感の演出に成功したといっても過言ではない。一旦、当時のブログを貼っておく。syu-m-5151.hatenablog.comでは、刮目せよ 俺のvimrc 2020 をset fenc=utf-8set nobackupset noswapfileset autoreadset hiddenset showcmdset numberset cursorlineset cursorcolumnset virtualedit=onemoreset smartindentset visualbellset showmatchset laststatus=2set wildmode=list:longestnnoremap j gjnnoremap k gkset titleset listset listchars=tab:»-,trail:-,eol:↲,extends:»,precedes:«,nbsp:%set list listchars=tab:\\▸\\-set expandtabset tabstop=4set shiftwidth=4set showmatch set ignorecaseset smartcaseset incsearchset wrapscanset hlsearchnmap <Esc><Esc> :nohlsearch<CR><Esc>set noautoindentset smartindent set viminfo='20,\\\"1000syntax enable\" Note: Skip initialization for vim-tiny or vim-small.if 0 | endifif &compatible  set nocompatible               \" Be iMprovedendif\" Required:set runtimepath+=~/.vim/bundle/neobundle.vim/\" Required:call neobundle#begin(expand('~/.vim/bundle/'))\" Let NeoBundle manage NeoBundle\" Required:NeoBundleFetch 'Shougo/neobundle.vim'\" Add or remove your Bundles here:\" Using Rust NeoBundle 'rust-lang/rust.vim'NeoBundle 'Shougo/neosnippet.vim'NeoBundle 'Shougo/neosnippet-snippets'NeoBundle 'tpope/vim-fugitive'NeoBundle 'ctrlpvim/ctrlp.vim'\" terraform and AnsibleNeoBundle 'flazz/vim-colorschemes'NeoBundle 'mrk21/yaml-vim'NeoBundle 'hashivim/vim-terraform'NeoBundle 'vim-syntastic/syntastic'NeoBundle 'juliosueiras/vim-terraform-completion'\" Using CNeoBundle 'kana/vim-operator-user'NeoBundle 'rhysd/vim-clang-format'\" Using GoNeoBundle 'fatih/vim-go'\" NeoBundle 'neoclide/coc.nvim'let g:go_highlight_functions = 1let g:go_highlight_methods = 1let g:go_highlight_structs = 1let g:go_highlight_operators = 1let g:go_highlight_build_constraints = 1let g:go_fmt_fail_silently = 1let g:go_fmt_autosave = 0syntax onautocmd FileType c ClangFormatAutoEnable\" My Bundles here:\" Refer to |:NeoBundle-examples|.\" Note: You don't set neobundle setting in .gvimrc!call neobundle#end()\" Required:filetype plugin indent on\" If there are uninstalled bundles found on startup,\" this will conveniently prompt you to install them.NeoBundleCheckhighlight Pmenu ctermbg=4highlight PmenuSel ctermbg=1highlight PMenuSbar ctermbg=4\" 補完ウィンドウの設定set completeopt=menuone\" 補完ウィンドウの設定set completeopt=menuone\" rsenseでの自動補完機能を有効化let g:rsenseUseOmniFunc = 1\" let g:rsenseHome = '/usr/local/lib/rsense-0.3'\" auto-ctagsを使ってファイル保存時にtagsファイルを更新let g:auto_ctags = 1\" 起動時に有効化let g:neocomplcache_enable_at_startup = 1\" 大文字が入力されるまで大文字小文字の区別を無視するlet g:neocomplcache_enable_smart_case = 1\" _(アンダースコア)区切りの補完を有効化let g:neocomplcache_enable_underbar_completion = 1let g:neocomplcache_enable_camel_case_completion  =  1\" 最初の補完候補を選択状態にするlet g:neocomplcache_enable_auto_select = 1\" ポップアップメニューで表示される候補の数let g:neocomplcache_max_list = 20\" シンタックスをキャッシュするときの最小文字長let g:neocomplcache_min_syntax_length = 3\" 補完の設定autocmd FileType ruby setlocal omnifunc=rubycomplete#Completeif !exists('g:neocomplete#force_omni_input_patterns')  let g:neocomplete#force_omni_input_patterns = {}endiflet g:neocomplete#force_omni_input_patterns.ruby = '[^.*\\t]\\.\\w*\\|\\h\\w*::'if !exists('g:neocomplete#keyword_patterns')        let g:neocomplete#keyword_patterns = {}endiflet g:neocomplete#keyword_patterns['default'] = '\\h\\w*'inoremap { {}<Left>inoremap {<Enter> {}<Left><CR><ESC><S-o>inoremap ( ()<Left>inoremap (<Enter> ()<Left><CR><ESC><S-o>inoremap [ []<Left>inoremap [<Enter> []<Left><CR><ESC><S-o>inoremap ( ()<ESC>iinoremap \"  \"\"<ESC>iinoremap '  ''<ESC>iautocmd FileType go :highlight goErr cterm=bold ctermfg=214autocmd FileType go :match goErr /\\<err\\>/\" Golang Serverif executable('gopls')    au User lsp_setup call lsp#register_server({        \\ 'name': 'gopls',        \\ 'cmd': {server_info->['gopls', '-mode', 'stdio']},        \\ 'whitelist': ['go'],        \\ })    autocmd BufWritePre *.go LspDocumentFormatSyncendifif executable('go-langserver')    au User lsp_setup call lsp#register_server({        \\ 'name': 'go-langserver',        \\ 'cmd': {server_info->['go-langserver', '-gocodecompletion']},        \\ 'whitelist': ['go'],        \\ })    autocmd BufWritePre *.go LspDocumentFormatSyncendif\" vim-lspif executable('golsp')  augroup LspGo    au!    autocmd User lsp_setup call lsp#register_server({        \\ 'name': 'go-lang',        \\ 'cmd': {server_info->['golsp', '-mode', 'stdio']},        \\ 'whitelist': ['go'],        \\ })    autocmd FileType go setlocal omnifunc=lsp#complete  augroup ENDendifcall plug#begin('~/.vim/plugged')Plug 'prabirshrestha/async.vim'Plug 'prabirshrestha/vim-lsp'Plug 'prabirshrestha/asyncomplete.vim'Plug 'prabirshrestha/asyncomplete-lsp.vim'Plug 'prabirshrestha/async.vim'Plug 'prabirshrestha/vim-lsp'Plug 'junegunn/fzf', { 'do': { -> fzf#install() } }Plug 'junegunn/fzf.vim'Plug 'natebosch/vim-lsc'Plug 'Shougo/defx.nvim'Plug 'roxma/nvim-yarp'Plug 'roxma/vim-hug-neovim-rpc'Plug 'SirVer/ultisnips'Plug 'honza/vim-snippets'Plug 'junegunn/fzf'Plug 'autozimu/LanguageClient-neovim', {    \\ 'branch': 'next',    \\ 'do': 'bash install.sh',    \\ }let g:lsp_async_completion = 1call plug#end()\" Track the engine.\" Snippets are separated from the engine. Add this if you want them:\" Trigger configuration. Do not use <tab> if you use https://github.com/Valloric/YouCompleteMe.let g:UltiSnipsExpandTrigger=\"<tab>\"let g:UltiSnipsJumpForwardTrigger=\"<c-b>\"let g:UltiSnipsJumpBackwardTrigger=\"<c-z>\"\" If you want :UltiSnipsEdit to split your window.let g:UltiSnipsEditSplit=\"vertical\"このvimrc はvimrc だけでは完結せずプラグインなどが入っているため入れる必要があるが何を入れればいいのかはよく分からんので適当にこの辺を読んでくれ。github.com","link":"https://syu-m-5151.hatenablog.com/entry/2020/07/24/211212","isoDate":"2020-07-24T12:12:12.000Z","dateMiliSeconds":1595592732000,"authorName":"nwiizo","authorId":"nwiizo"},{"title":"kyaml2go を使って yaml から簡単にGolang のコードを生成して遊びたい","contentSnippet":"前書きcurl をgolang で行いたい時にはcurl-to-Go を利用することができる。ConoHa のAPIを利用して トークン発行 を発行するしたい場合には以下のようなcurl を実行する必要がある トークン発行 - Identity API v2.0 / ConoHa API を参照curl -i -X POST \\-H \"Accept: application/json\" \\-d '{\"auth\":{\"passwordCredentials\":{\"username\":\"ConoHa\",\"password\":\"paSSword123456#$%\"},\"tenantId\":\"487727e3921d44e3bfe7ebb337bf085e\"}}' \\https://identity.tyo1.conoha.io/v2.0/tokenscurl-to-Go を利用すると以下のようなコードが生成されて労力を最小限にすることができる。労力が最小なことは嬉しい。ちなみにCLI も利用できるのでかなり使っている。// Generated by curl-to-Go: https://mholt.github.io/curl-to-gobody := strings.NewReader(`{\"auth\":{\"passwordCredentials\":{\"username\":\"ConoHa\",\"password\":\"paSSword123456#$%\"},\"tenantId\":\"487727e3921d44e3bfe7ebb337bf085e\"}}`)req, err := http.NewRequest(\"POST\", \"https://identity.tyo1.conoha.io/v2.0/tokens\", body)if err != nil {    // handle err}req.Header.Set(\"Accept\", \"application/json\")req.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")resp, err := http.DefaultClient.Do(req)if err != nil {    // handle err}defer resp.Body.Close()まぁこの前書きはマジで何も関係ない。概要kubernetes API にアクセスする時には様々な方法がある。kubectl や curl などでも行う事ができる。でも、kubectl 使うことが一般的だとは思う。しかし、もう少し複雑な事をさせたいと思ったらのなら何かしらのプログラミング言語で実施したくなります。Kubernetes APIにアクセスするにはk8s.io/client-goを使うのはわりと自然な流れかと思いますが。この場合client-goを使うコードを1から書いてもよいですが、kyaml2goを使うと、yamlファイルからclient-goを利用するコードを生成してれるので特定の操作を簡単に行う事ができます。下記のようなyaml があったとしたら そのyaml にcreate update  get delete してくれるコードを生成してくれます。client-go について詳しく学習したい場合には Programming Kuberentes の三章であるBasics of client-go を読んでください。apiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80create を選択した場合(Web UI であるkyaml2go: Kubernetes client-go code generator から引用)// Auto-generated by kyaml2go - https://github.com/PrasadG193/kyaml2gopackage mainimport (    \"fmt\"    appsv1 \"k8s.io/api/apps/v1\"    corev1 \"k8s.io/api/core/v1\"    metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"    \"k8s.io/client-go/kubernetes\"    \"k8s.io/client-go/tools/clientcmd\"    \"k8s.io/client-go/util/homedir\"    \"os\"    \"path/filepath\")func main() {    // Create client    var kubeconfig string    kubeconfig, ok := os.LookupEnv(\"KUBECONFIG\")    if !ok {        kubeconfig = filepath.Join(homedir.HomeDir(), \".kube\", \"config\")    }    config, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfig)    if err != nil {        panic(err)    }    clientset, err := kubernetes.NewForConfig(config)    if err != nil {        panic(err)    }    kubeclient := clientset.AppsV1().Deployments(\"default\")    // Create resource object    object := &appsv1.Deployment{        TypeMeta: metav1.TypeMeta{            Kind:       \"Deployment\",            APIVersion: \"apps/v1\",        },        ObjectMeta: metav1.ObjectMeta{            Name: \"nginx-deployment\",            Labels: map[string]string{                \"app\": \"nginx\",            },        },        Spec: appsv1.DeploymentSpec{            Replicas: ptrint32(3),            Selector: &metav1.LabelSelector{                MatchLabels: map[string]string{                    \"app\": \"nginx\",                },            },            Template: corev1.PodTemplateSpec{                ObjectMeta: metav1.ObjectMeta{                    Labels: map[string]string{                        \"app\": \"nginx\",                    },                },                Spec: corev1.PodSpec{                    Containers: []corev1.Container{                        corev1.Container{                            Name:  \"nginx\",                            Image: \"nginx:1.14.2\",                            Ports: []corev1.ContainerPort{                                corev1.ContainerPort{                                    HostPort:      0,                                    ContainerPort: 80,                                },                            },                            Resources: corev1.ResourceRequirements{},                        },                    },                },            },            Strategy:        appsv1.DeploymentStrategy{},            MinReadySeconds: 0,        },    }    // Manage resource    _, err = kubeclient.Create(object)    if err != nil {        panic(err)    }    fmt.Println(\"Deployment Created successfully!\")}func ptrint32(p int32) *int32 {    return &p}CLIもあるのでCLI をインストールして実際に見ていく遊ぶ環境情報$ go versiongo version go1.14.5 linux/amd64$ docker version                                                                                                                                                                                                                  4290msClient: Version:           19.03.8 API version:       1.40 Go version:        go1.13.8 Git commit:        afacb8b7f0 Built:             Tue Jun 23 22:26:12 2020 OS/Arch:           linux/amd64 Experimental:      falseServer: Engine:  Version:          19.03.11  API version:      1.40 (minimum version 1.12)  Go version:       go1.13.12  Git commit:       77e06fd  Built:            Mon Jun  8 20:24:59 2020  OS/Arch:          linux/amd64  Experimental:     false containerd:  Version:          v1.2.13  GitCommit:        7ad184331fa3e55e52b890ea95e65ba581ae3429 runc:  Version:          1.0.0-rc10  GitCommit: docker-init:  Version:          0.18.0  GitCommit:        fec3683$ kind --version # https://kind.sigs.k8s.io/docs/user/quick-start/kind version 0.8.1環境構築クラスタ-の構築$ kind create cluster --name kyaml2goCreating cluster \"kyaml2go\" ... ✓ Ensuring node image (kindest/node:v1.18.2) 🖼 ✓ Preparing nodes 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾Set kubectl context to \"kind-kyaml2go\"You can now use your cluster with:kubectl cluster-info --context kind-kyaml2goHave a nice day!クラスタ-の確認$ kind get clusterskyaml2go$ kind get kubeconfig --name kyaml2go > kubeconfig.yaml$ kubectl version  --kubeconfig=kubeconfig.yamlClient Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.0\", GitCommit:\"9e991415386e4cf155a24b1da15becaa390438d8\", GitTreeState:\"clean\", BuildDate:\"2020-03-25T14:58:59Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"linux/amd64\"}Server Version: version.Info{Major:\"1\", Minor:\"18\", GitVersion:\"v1.18.2\", GitCommit:\"52c56ce7a8272c798dbc29846288d7cd9fbae032\", GitTreeState:\"clean\", BuildDate:\"2020-04-30T20:19:45Z\", GoVersion:\"go1.13.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}cli のインストール git clone https://github.com/PrasadG193/kyaml2go.git  cd kyaml2go make$(shell go env GOPATH)/bin/kyaml2go 以下にコマンドが生成されているのでそれにyaml を下記のように食わせる(牡蛎食べたい 腹減った)(サンプルには下記のような書き方が書いてあるが複数のアクションを同時に食わせるのは恐らく対応してない)kyaml2go create/get/update/delete -f /path/to/resource_specs.yaml 設定するファイルapiVersion: apps/v1kind: Deploymentmetadata:  name: nginx-deployment  labels:    app: nginxspec:  replicas: 3  selector:    matchLabels:      app: nginx  template:    metadata:      labels:        app: nginx    spec:      containers:      - name: nginx        image: nginx:1.14.2        ports:        - containerPort: 80kyaml2go の実行kyaml2go create -f nginx-deployment.yaml > create-nginx-deployment.gocreate-nginx-deployment.go 生成したものを書きだす。~/.config/fish/config.fish などで set KUBECONFIG ./kubeconfig.yaml:~/.kube/config を設定するのでカレントディレクトリにkubeconfig.yaml があった場合には$KUBECONFIGという環境変数にはそちらが優先されて設定されるので便利がいいですね。// Auto-generated by kyaml2go - https://github.com/PrasadG193/kyaml2gopackage mainimport (        \"fmt\"        appsv1 \"k8s.io/api/apps/v1\"        corev1 \"k8s.io/api/core/v1\"        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"        \"k8s.io/client-go/kubernetes\"        \"k8s.io/client-go/tools/clientcmd\"        \"k8s.io/client-go/util/homedir\"        \"os\"        \"path/filepath\")func main() {        // Create client        var kubeconfig string        kubeconfig, ok := os.LookupEnv(\"KUBECONFIG\")        if !ok {                kubeconfig = filepath.Join(homedir.HomeDir(), \".kube\", \"config\")        }        config, err := clientcmd.BuildConfigFromFlags(\"\", kubeconfig)        if err != nil {                panic(err)        }        clientset, err := kubernetes.NewForConfig(config)        if err != nil {                panic(err)        }        kubeclient := clientset.AppsV1().Deployments(\"default\")        // Create resource object        object := &appsv1.Deployment{                TypeMeta: metav1.TypeMeta{                        Kind:       \"Deployment\",                        APIVersion: \"apps/v1\",                },                ObjectMeta: metav1.ObjectMeta{                        Name: \"nginx-deployment\",                        Labels: map[string]string{                                \"app\": \"nginx\",                        },                },                Spec: appsv1.DeploymentSpec{                        Replicas: ptrint32(3),                        Selector: &metav1.LabelSelector{                                MatchLabels: map[string]string{                                        \"app\": \"nginx\",                                },                        },                        Template: corev1.PodTemplateSpec{                                ObjectMeta: metav1.ObjectMeta{                                        Labels: map[string]string{                                                \"app\": \"nginx\",                                        },                                },                                Spec: corev1.PodSpec{                                        Containers: []corev1.Container{                                                corev1.Container{                                                        Name:  \"nginx\",                                                        Image: \"nginx:1.14.2\",                                                        Ports: []corev1.ContainerPort{                                                                corev1.ContainerPort{                                                                        HostPort:      0,                                                                        ContainerPort: 80,                                                                },                                                        },                                                        Resources: corev1.ResourceRequirements{},                                                },                                        },                                },                        },                        Strategy:        appsv1.DeploymentStrategy{},                        MinReadySeconds: 0,                },        }        // Manage resource        _, err = kubeclient.Create(object)        if err != nil {                panic(err)        }        fmt.Println(\"Deployment Created successfully!\")}func ptrint32(p int32) *int32 {        return &p}clusterへの実行create-nginx-deployment.go を実行してみたが go 1.14のgo run で乱雑に入るパッケージのいくつかが対応していないのでkyaml2goのgo.mod を参照して入れなおす$ go run create-nginx-deployment.goDeployment Created successfully!$ go run get-nginx-deployment.goFound object : &Deployment{ObjectMeta:{nginx-deployment  default /apis/apps/v1/namespaces/default/deployments/nginx-deployment 7a7fd262-09d1-49ef-89d2-5116c68d4e9e 53121316 1 2020-07-21 13:01:11 +0000 UTC <nil> <nil> map[app:nginx] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*3,Selector:&v1.LabelSelector{MatchLabels:map[string]string{app: nginx,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[app:nginx] map[] [] []  []} {[] [] [{nginx nginx:1.14.2 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc000584068 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%,MaxSurge:25%,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:3,UpdatedReplicas:3,AvailableReplicas:3,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2020-07-21 13:01:23 +0000 UTC,LastTransitionTime:2020-07-21 13:01:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet \"nginx-deployment-7fd6966748\" has successfully progressed.,LastUpdateTime:2020-07-21 13:01:23 +0000 UTC,LastTransitionTime:2020-07-21 13:01:11 +0000 UTC,},},ReadyReplicas:3,CollisionCount:nil,},}resourceが生成されているのが確認できたと思うkubectl でも同様に確認できたので$ kubectl get pod                                                                                                                                                                                                                      NAME                                READY   STATUS    RESTARTS   AGEnginx-deployment-7fd6966748-9km6c   1/1     Running   0          9m26snginx-deployment-7fd6966748-c9sng   1/1     Running   0          9m26snginx-deployment-7fd6966748-thk76   1/1     Running   0          9m26s削除しておく$ go run delete-nginx-deployment.go Deployment Deleted successfully!ソースコードなどはココに載せておきます。github.com最後に特にないけど Kind で構築したclusterを削除するkind get clusterskyaml2gokind delete cluster --name kyaml2goDeleting cluster \"kyaml2go\" ...","link":"https://syu-m-5151.hatenablog.com/entry/2020/07/21/222843","isoDate":"2020-07-21T13:28:43.000Z","dateMiliSeconds":1595338123000,"authorName":"nwiizo","authorId":"nwiizo"}]},"__N_SSG":true}